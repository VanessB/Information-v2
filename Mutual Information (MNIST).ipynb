{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information (MNIST)\n",
    "\n",
    "Эксперименты с оценкой энтропии для данных рукописных цифр."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PuuYTaasyqC"
   },
   "source": [
    "# Преамбула"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J20_kxWGua1g"
   },
   "source": [
    "## Библиотеки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knCL6YcRtDSI"
   },
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKI49Wt7s1ZH",
    "outputId": "00b0c2c1-a14d-4681-b725-966bd57299dc"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81zMD0EitGlJ"
   },
   "source": [
    "### Math, Numpy, Scipy, Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TrbCI8-s4re"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as sps\n",
    "import scipy.linalg as spl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GwNnxNiDgYw"
   },
   "source": [
    "### Matplotlib, Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0SjY2FyDgiY"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWH6JIAJtbs3"
   },
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rb9G9YxtfD3"
   },
   "outputs": [],
   "source": [
    "# Деревья.\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "# Метрика.\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "# Метод главных компонент.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Выбор модели по кросс-валидации (поиск по сетке).\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLVaeOz9tbwI"
   },
   "source": [
    "### Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5R5GjsuMtPe4"
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "n_jobs = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCTOH5CQuULh"
   },
   "source": [
    "### OS, shutil, Json, CSV, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwC7bZldt4qY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import csv\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9zP5A4nufLs"
   },
   "source": [
    "## Вспомогательное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flPbS2ebuY7h"
   },
   "outputs": [],
   "source": [
    "# Информация об опыте.\n",
    "info = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RyvEh9bulAV"
   },
   "outputs": [],
   "source": [
    "def normalize_uint8(data, label):\n",
    "    \"\"\"Нормализация: `uint8` -> `float32`.\"\"\"\n",
    "    return tf.cast(data, tf.float32) / 255.0, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5xA0W2DvIBq"
   },
   "outputs": [],
   "source": [
    "def imshow_array(array):\n",
    "    \"\"\"Отображение массива нормированных пикселей.\"\"\"\n",
    "    plt.axis('off')\n",
    "    plt.imshow((255.0 * array).astype(np.uint8), cmap=plt.get_cmap(\"gray\"), vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5OjeuhXvP6O"
   },
   "outputs": [],
   "source": [
    "def dataset_Y_to_X(X, Y):\n",
    "    \"\"\"Поменять у датасета пары (X, Y) на (X, X) (нужно, например, для обучения автоэнкодера).\"\"\"\n",
    "    return X, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qo0MXHDowRLZ"
   },
   "outputs": [],
   "source": [
    "def concave_loss(y_true, y_pred):\n",
    "    \"\"\"Вогнутая функция потерь, дающая более четкие изображения при обучении.\"\"\"\n",
    "    delta = tf.keras.backend.abs(y_true - y_pred)\n",
    "    squared = tf.keras.backend.square(y_true - y_pred)\n",
    "    return tf.keras.backend.mean(delta - 0.5 * squared, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9o4czLSwxib"
   },
   "source": [
    "## Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aSdv1fxZwt8N",
    "outputId": "c08303d3-5680-4b43-f4f0-26fa82612a4a"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Путь к папке с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tneJ2JaEwztO"
   },
   "outputs": [],
   "source": [
    "#path = \"/content/drive/My Drive/Information_v2/\"\n",
    "path = os.path.abspath(os.getcwd()) + \"/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_path = path + \"mutual_information/MNIST/\"\n",
    "models_path = experiments_path + \"models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_shape = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полный набор данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_full_train, ds_full_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_full_train = ds_full_train.map(normalize_uint8, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_full_test  = ds_full_test.map(normalize_uint8, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_full_train.take(60000)\n",
    "ds_train = np.array([sample for sample in ds_train])\n",
    "\n",
    "ds_test  = ds_full_test.take(60000)\n",
    "ds_test  = np.array([sample for sample in ds_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_X = ds_train[:,0]\n",
    "ds_test_X  = ds_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_Y = ds_train[:,1]\n",
    "ds_test_Y = ds_test[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификатор изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировочные и тестовые наборы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_batch_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_cl_train, ds_cl_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_cl_train = ds_cl_train.map(normalize_uint8, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_cl_train = ds_cl_train.cache()\n",
    "ds_cl_train = ds_cl_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_cl_train = ds_cl_train.batch(cl_batch_size)\n",
    "ds_cl_train = ds_cl_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_cl_test = ds_cl_test.map(normalize_uint8, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_cl_test = ds_cl_test.batch(cl_batch_size)\n",
    "ds_cl_test = ds_cl_test.cache()\n",
    "ds_cl_test = ds_cl_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_classifier(shape_input):\n",
    "    # Инициализация весов.\n",
    "    init = tf.keras.initializers.RandomNormal(stddev = 0.02)\n",
    "\n",
    "    # Входные данные генератора / выборки.\n",
    "    input_layer = tf.keras.layers.Input(shape_input)\n",
    "    next_layer = input_layer\n",
    "    next_layer = tf.keras.layers.GaussianNoise(1e-2, name='AGN_0')(next_layer)\n",
    "\n",
    "    # 1 блок слоёв.  \n",
    "    next_layer = tfa.layers.SpectralNormalization(name='SN_1', tf.keras.layers.Conv2D(\n",
    "        filters = 16, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init))(next_layer)\n",
    "    \n",
    "    #next_layer = tf.keras.layers.BatchNormalization()(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dropout(0.1, name='DO_1')(next_layer)\n",
    "    next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), padding = 'same')(next_layer)\n",
    "    next_layer = tf.keras.layers.GaussianNoise(1e-2, name='AGN_1')(next_layer)\n",
    "\n",
    "    output_layer_1 = next_layer\n",
    "\n",
    "    # 2 блок слоёв. \n",
    "    next_layer = tfa.layers.SpectralNormalization(name='SN_2', tf.keras.layers.Conv2D(\n",
    "        filters = 8, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init))(next_layer)\n",
    "    \n",
    "    next_layer = tf.keras.layers.BatchNormalization(name='BN_2')(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dropout(0.1, name='DO_2')(next_layer)\n",
    "    next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), padding = 'same')(next_layer)\n",
    "    next_layer = tf.keras.layers.GaussianNoise(1e-2, name='AGN_2')(next_layer)\n",
    "\n",
    "    output_layer_2 = next_layer\n",
    "\n",
    "    # 3 блок слоёв.\n",
    "    next_layer = tfa.layers.SpectralNormalization(name='SN_3', tf.keras.layers.Conv2D(\n",
    "        filters = 4, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init))(next_layer)\n",
    "    next_layer = tf.keras.layers.BatchNormalization(name='BN_3')(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dropout(0.1, name='DO_3')(next_layer)\n",
    "    next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), padding = 'same')(next_layer)\n",
    "    next_layer = tf.keras.layers.GaussianNoise(1e-2, name='AGN_3')(next_layer)\n",
    "\n",
    "    output_layer_3 = next_layer\n",
    "    \n",
    "    # 4 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.GaussianNoise(0.05, name='AGN_4')(next_layer)\n",
    "    #next_layer = tf.keras.layers.Conv2D(filters = 16, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init)(next_layer)\n",
    "    #next_layer = tf.keras.layers.BatchNormalization(name='BA_3')(next_layer)\n",
    "    #next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    #next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "    #next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), padding = 'same')(next_layer)\n",
    "\n",
    "    #output_layer_4 = tf.keras.layers.Activation(tf.keras.activations.sigmoid)(next_layer)\n",
    "\n",
    "    # Вывод.\n",
    "    next_layer = tf.keras.layers.Flatten()(next_layer)\n",
    "    next_layer = tfa.layers.SpectralNormalization(tf.keras.layers.Dense(10))(next_layer)\n",
    "    output_layer = tf.keras.layers.Activation('softmax')(next_layer)\n",
    "\n",
    "    # Модель.\n",
    "    model = tf.keras.models.Model(input_layer, output_layer)\n",
    "    debug_model = tf.keras.models.Model([input_layer], [output_layer_1, output_layer_2, output_layer_3])\n",
    "\n",
    "    # Компиляция модели.\n",
    "    opt = tf.keras.optimizers.Adam(lr = 1e-3)\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = opt, loss_weights = [1.0], metrics=['accuracy'])\n",
    "    return model, debug_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка модели.\n",
    "#classifier = tf.keras.models.load_model(models_path + \"/classifier/classifier.h5\")\n",
    "#debug_classifier = tf.keras.models.load_model(models_path + \"/classifier/debug_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier, debug_classifier = convolutional_classifier(mnist_shape)\n",
    "# Сводка по модели.\n",
    "classifier.summary()\n",
    "# Отрисовка модели.\n",
    "#tf.keras.utils.plot_model(classifier, show_shapes = True, show_layernames = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier.fit(\n",
    "    ds_cl_train,\n",
    "    epochs=300,\n",
    "    validation_data=ds_cl_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(models_path + \"/classifier/classifier.h5\")\n",
    "debug_classifier.save(models_path + \"/classifier/debug_classifier.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автокодировщик для изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировочные и тестовые наборы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_batch_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_ae_train, ds_ae_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ae_train = ds_ae_train.map(normalize_uint8, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_ae_train = ds_ae_train.map(dataset_Y_to_X, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_ae_train = ds_ae_train.cache()\n",
    "ds_ae_train = ds_ae_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_ae_train = ds_ae_train.batch(ae_batch_size)\n",
    "ds_ae_train = ds_ae_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ae_test = ds_ae_test.map(normalize_uint8, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_ae_test = ds_ae_test.map(dataset_Y_to_X, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_ae_test = ds_ae_test.batch(ae_batch_size)\n",
    "ds_ae_test = ds_ae_test.cache()\n",
    "ds_ae_test = ds_ae_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Автокодировщик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# РАЗМЕРНОСТЬ КОДА.\n",
    "# #\n",
    "# #\n",
    "\n",
    "codes_dim_X = 10 # MNSIT\n",
    "\n",
    "# #\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_autoencoder(shape_input, dimension):\n",
    "    # Инициализация весов.\n",
    "    init = tf.keras.initializers.RandomNormal(stddev = 1.0)\n",
    "\n",
    "    # Входные данные генератора / выборки.\n",
    "    input_layer = tf.keras.layers.Input(shape_input)\n",
    "    next_layer = input_layer\n",
    "\n",
    "    # 1 блок слоёв.\n",
    "    next_layer = tf.keras.layers.GaussianNoise(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.Conv2D(filters = 12, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.BatchNormalization()(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    #next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), padding = 'same')(next_layer)\n",
    "\n",
    "    # 2 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.GaussianNoise(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.Conv2D(filters = 18, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.BatchNormalization()(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), padding = 'same')(next_layer)\n",
    "\n",
    "    # 3 блок слоёв.\n",
    "    next_layer = tf.keras.layers.GaussianNoise(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.Conv2D(filters = 27, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.BatchNormalization()(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), padding = 'same')(next_layer)\n",
    "\n",
    "    # Бутылочное горлышко.\n",
    "    next_layer = tf.keras.layers.Flatten()(next_layer)\n",
    "    next_layer = tf.keras.layers.Dense(dimension)(next_layer)\n",
    "    bottleneck = tf.keras.layers.Activation('tanh')(next_layer)\n",
    "\n",
    "    # Модель кодировщика.\n",
    "    encoder = tf.keras.Model(input_layer, bottleneck)\n",
    "\n",
    "    # Начало модели декодировщика.\n",
    "    input_code_layer = tf.keras.layers.Input((dimension))\n",
    "    next_layer = input_code_layer\n",
    "\n",
    "    # 3 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.GaussianNoise(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dense(4*4*27)(next_layer)\n",
    "    next_layer = tf.keras.layers.Reshape((4, 4, 27))(next_layer)\n",
    "    next_layer = tf.keras.layers.BatchNormalization()(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    #next_layer = tf.keras.layers.Dropout(0.2)(next_layer)\n",
    "\n",
    "    # 2 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.GaussianNoise(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.UpSampling2D(size=(2, 2))(next_layer)\n",
    "    next_layer = tf.keras.layers.Conv2D(filters = 18, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.Cropping2D(cropping=((0, 1), (0, 1)))(next_layer)\n",
    "    next_layer = tf.keras.layers.BatchNormalization()(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    #next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "\n",
    "    # 1 блок слоёв.\n",
    "    next_layer = tf.keras.layers.GaussianNoise(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.UpSampling2D(size=(2, 2))(next_layer)\n",
    "    next_layer = tf.keras.layers.Conv2D(filters = 12, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.BatchNormalization()(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "\n",
    "    # 0 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.GaussianNoise(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.UpSampling2D(size=(2, 2))(next_layer)\n",
    "    next_layer = tf.keras.layers.Conv2D(filters = 1, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.BatchNormalization()(next_layer)\n",
    "    next_layer = tf.keras.layers.Activation('sigmoid')(next_layer)\n",
    "    #next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "\n",
    "    output_layer = next_layer\n",
    "\n",
    "    # Модель.\n",
    "    decoder = tf.keras.models.Model(input_code_layer, output_layer) # Декодировщик.\n",
    "    autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "\n",
    "    # Компиляция модели.\n",
    "    opt = tf.keras.optimizers.Adam(lr = 5e-3)\n",
    "    autoencoder.compile(loss = concave_loss, optimizer = opt, loss_weights = [1.0])\n",
    "    return encoder, decoder, autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_X = tf.keras.models.load_model(models_path + \"autoencoder/encoder_X.h5\")\n",
    "#decoder_X = tf.keras.models.load_model(models_path + \"autoencoder/decoder_X.h5\")\n",
    "#autoencoder_X = tf.keras.Sequential([encoder_X, decoder_X])\n",
    "#autoencoder_X.compile(loss = concave_loss, optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3), loss_weights = [1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_X, decoder_X, autoencoder_X = cnn_autoencoder(mnist_shape, codes_dim_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder_X.fit(\n",
    "    ds_ae_train,\n",
    "    epochs=300,\n",
    "    validation_data=ds_ae_test,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_X.compile(loss = concave_loss, optimizer = tf.keras.optimizers.Adam(lr = 1e-3), loss_weights = [1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_X.fit(\n",
    "    ds_ae_train,\n",
    "    epochs=100,\n",
    "    validation_data=ds_ae_test,\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение моделей.\n",
    "autoencoder_X.save(models_path + \"/autoencoder/autoencoder_X.h5\")\n",
    "encoder_X.save(models_path + \"/autoencoder/encoder_X.h5\")\n",
    "decoder_X.save(models_path + \"/autoencoder/decoder_X.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка взаимной информации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Номер исследуемого слоя.\n",
    "layer_index = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перестройка исследуемой модели без выпадения и нормализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выключение выпадение.\n",
    "debug_classifier.get_layer('DO_1').rate = 0.0\n",
    "debug_classifier.get_layer('DO_2').rate = 0.0\n",
    "debug_classifier.get_layer('DO_3').rate = 0.0\n",
    "\n",
    "# Выключение нормализации по батчам.\n",
    "debug_classifier.get_layer('BN_2').trainable = False\n",
    "debug_classifier.get_layer('BN_3').trainable = False\n",
    "\n",
    "# Выключение спектральной нормализации.\n",
    "debug_classifier.get_layer('SN_1').trainable = False\n",
    "debug_classifier.get_layer('SN_2').trainable = False\n",
    "debug_classifier.get_layer('SN_3').trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Клонирование модели\n",
    "old_classifier = classifier\n",
    "classifier = tf.keras.models.clone_model(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка весов\n",
    "classifier.set_weights(old_classifier.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_example = ds_full_train.take(3)\n",
    "for sample in mnist_example:\n",
    "    image, label = sample[0].numpy(), sample[1].numpy()\n",
    "    imshow_array(image[:, :, 0])\n",
    "    plt.show()\n",
    "    print(\"Label: %d\" % label)\n",
    "\n",
    "    # Предсказание.\n",
    "    print(classifier.predict(np.array([image]))[0])\n",
    "\n",
    "    # Отрисовка слоёв.\n",
    "   \n",
    "    debug = debug_classifier(np.array([image]), training=True)\n",
    "    for output in debug:\n",
    "        for element in output:\n",
    "            element = np.swapaxes(element, 0, 2)\n",
    "            element = np.swapaxes(element, 1, 2)\n",
    "\n",
    "            i = 0\n",
    "            n = len(element)\n",
    "            for filter in element:\n",
    "                plt.subplot(1, n, 1 + i)\n",
    "                imshow_array(filter)\n",
    "                i += 1\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получение значений слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_splitted = tf.split(tf.stack(ds_train_X), 10)\n",
    "_layer_predicted_train = tf.concat([debug_classifier(_splitted[i], training=False)[layer_index - 1] for i in range(10)], 0)\n",
    "\n",
    "_splitted = tf.split(tf.stack(ds_test_X), 10)\n",
    "_layer_predicted_test = tf.concat([debug_classifier(_splitted[i], training=False)[layer_index - 1] for i in range(10)], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_L = np.array([_layer_predicted_train[i].numpy().flatten() for i in range(_layer_predicted_train.shape[0])])\n",
    "ds_test_L  = np.array([_layer_predicted_test[i].numpy().flatten() for i in range(_layer_predicted_test.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pRyKwjKCqml"
   },
   "source": [
    "### Путь к результатам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBi5v6hSAk1K"
   },
   "outputs": [],
   "source": [
    "dataset_path = experiments_path + \"layer_\" + str(layer_index) + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2coUXxYyAVx"
   },
   "source": [
    "## Автокодировщик\n",
    "\n",
    "Сжатие данных предлагается делать автокодировщиком.\n",
    "Для архитектуры специфицируется только формат входных данных, а также размерность внутреннего представления (кодов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aa1TECWvxuEF"
   },
   "outputs": [],
   "source": [
    "# РАЗМЕРНОСТЬ КОДА.\n",
    "# #\n",
    "# #\n",
    "\n",
    "codes_dim_L = 4  # Слой.\n",
    "\n",
    "# #\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3McJCha_b_G"
   },
   "outputs": [],
   "source": [
    "# Число эпох для обучения.\n",
    "autoencoders_epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOu7nB9gx9vH"
   },
   "outputs": [],
   "source": [
    "full_path = dataset_path + \"autoencoders/\"\n",
    "os.makedirs(full_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lwIjSKL_gMv"
   },
   "outputs": [],
   "source": [
    "info['autoencoders_epochs'] = autoencoders_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45W1qKkoB90y"
   },
   "source": [
    "### Автокодировщик для слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2475m-kB1mm"
   },
   "outputs": [],
   "source": [
    "batch_normalizationdef dense_autoencoder(shape_input, dimension):\n",
    "    # Инициализация весов.\n",
    "    init = tf.keras.initializers.RandomNormal(stddev = 0.02)\n",
    "\n",
    "    # Входные данные генератора / выборки.\n",
    "    input_layer = tf.keras.layers.Input(shape_input)\n",
    "    next_layer = input_layer\n",
    "    next_layer = tf.keras.layers.GaussianNoise(0.02)(next_layer)\n",
    "\n",
    "    # 1 блок слоёв.\n",
    "    next_layer = tfa.layers.SpectralNormalization(tf.keras.layers.Dense(512, kernel_initializer = init),\n",
    "                                                  power_iterations = 3)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "\n",
    "    # 2 блок слоёв.\n",
    "    next_layer = tfa.layers.SpectralNormalization(tf.keras.layers.Dense(256, kernel_initializer = init),\n",
    "                                                  power_iterations = 3)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "    \n",
    "    # 3 блок слоёв.\n",
    "    #next_layer = tfa.layers.SpectralNormalization(tf.keras.layers.Dense(128, kernel_initializer = init),\n",
    "    #                                              power_iterations = 3)(next_layer)\n",
    "    #next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    #next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "    \n",
    "    # 4 блок слоёв.\n",
    "    #next_layer = tfa.layers.SpectralNormalization(tf.keras.layers.Dense(32, kernel_initializer = init),\n",
    "    #                                              power_iterations = 3)(next_layer)\n",
    "    #next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    #next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "    \n",
    "    # Бутылочное горлышко.\n",
    "    next_layer = tfa.layers.SpectralNormalization(tf.keras.layers.Dense(dimension),\n",
    "                                                  power_iterations = 3)(next_layer)\n",
    "    bottleneck = tf.keras.layers.Activation('tanh', name='bottleneck')(next_layer)\n",
    "\n",
    "    # Модель кодировщика.\n",
    "    encoder = tf.keras.Model(input_layer, bottleneck)\n",
    "\n",
    "    # Начало модели декодировщика.\n",
    "    input_code_L = tf.keras.layers.Input((dimension))\n",
    "    next_layer = input_code_L\n",
    "    \n",
    "    # 4 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.Dense(32, kernel_initializer = init)(next_layer)\n",
    "    #next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "\n",
    "    # 3 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.Dense(128, kernel_initializer = init)(next_layer)\n",
    "    #next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    \n",
    "    # 2 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Dense(256, kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "\n",
    "    # 1 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Dense(512, kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    \n",
    "    # 0 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Dense(shape_input[0])(next_layer) # Подразумевается, что вход - всё равно вектор.\n",
    "    #next_layer = tf.keras.layers.Activation('tanh')(next_layer)\n",
    "    \n",
    "    output_layer = next_layer\n",
    "    \n",
    "    # Модель.\n",
    "    decoder = tf.keras.models.Model(input_code_L, output_layer) # Декодировщик.\n",
    "    autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "\n",
    "    # Компиляция модели.\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 2e-3)\n",
    "    autoencoder.compile(loss = 'mse', optimizer = opt)\n",
    "    \n",
    "    return encoder, decoder, autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzOjEmMjCCWS"
   },
   "source": [
    "### Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmzV6titB6zT"
   },
   "outputs": [],
   "source": [
    "#encoder_L = tf.keras.models.load_model(full_path + \"encoder_L.h5\")\n",
    "#decoder_L = tf.keras.models.load_model(full_path + \"decoder_L.h5\")\n",
    "#autoencoder_L = tf.keras.Sequential([encoder_L, decoder_L])\n",
    "#autoencoder.compile(loss = 'mse', optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3), loss_weights = [1.0])\n",
    "\n",
    "#with open(full_path + 'info.json', 'r') as fp:\n",
    "#    info = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ld3eyuWPCHtW"
   },
   "outputs": [],
   "source": [
    "encoder_L, decoder_L, autoencoder_L = dense_autoencoder((ds_train_L.shape[1],), codes_dim_L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder_L.fit(\n",
    "    ds_train_L,\n",
    "    ds_train_L,\n",
    "    epochs=autoencoders_epochs,\n",
    "    validation_data=(ds_test_L, ds_test_L),\n",
    "    batch_size=ds_train_L.shape[0] // 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_L.compile(loss = 'mse', optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder_L.fit(\n",
    "    ds_train_L,\n",
    "    ds_train_L,\n",
    "    epochs=1000,\n",
    "    validation_data=(ds_test_L, ds_test_L),\n",
    "    batch_size=ds_train_L.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQxF6KcgNag-",
    "outputId": "21f60548-9dd3-4430-a27e-3e871a431cfe"
   },
   "outputs": [],
   "source": [
    "# Сохранение моделей.\n",
    "autoencoder_L.save(full_path + \"autoencoder_L.h5\")\n",
    "encoder_L.save(full_path + \"encoder_L.h5\")\n",
    "decoder_L.save(full_path + \"decoder_L.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuImkbabNcqj"
   },
   "outputs": [],
   "source": [
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPMRQmWmNjIS"
   },
   "source": [
    "### Получение кодов всех элементов набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вход классификатора\n",
    "_splitted = tf.split(tf.stack(ds_train_X), 10)\n",
    "codes_X = tf.concat([encoder_X(_splitted[i], training=False) for i in range(10)], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mksJnyPiNkhD"
   },
   "outputs": [],
   "source": [
    "# Выход слоя\n",
    "codes_L = np.array(encoder_L.predict(ds_train_L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Совместный датасет для входа классификатора и выхода слоя\n",
    "codes_X_L = np.concatenate((codes_X, codes_L), 1)\n",
    "codes_X_layerdim = codes_dim_L + codes_dim_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка\n",
    "print(np.linalg.norm(ds_train_L[0] -\n",
    "                     debug_classifier(np.expand_dims(ds_train_X[0], 0), training=False)[2].numpy().flatten()))\n",
    "\n",
    "print(np.linalg.norm(ds_train_L[1] -\n",
    "                     debug_classifier(np.expand_dims(ds_train_X[0], 0), training=False)[2].numpy().flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EMu1RqxN7vj"
   },
   "outputs": [],
   "source": [
    "PCA_codes_X = PCA(n_components=codes_dim_X, whiten=True)\n",
    "codes_pca_X = np.array(PCA_codes_X.fit_transform(codes_X))\n",
    "\n",
    "PCA_codes_L = PCA(n_components=codes_dim_L, whiten=True)\n",
    "codes_pca_L = np.array(PCA_codes_L.fit_transform(codes_L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_codes_X_L = PCA(n_components=codes_X_layerdim, whiten=True)\n",
    "codes_pca_X_L = np.array(PCA_codes_X_L.fit_transform(codes_X_L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = sns.pairplot(pd.DataFrame(codes_pca_X_L[0:1000]), height = 2.0, aspect=1.6,\n",
    "                      plot_kws=dict(edgecolor=\"k\", linewidth=0.0, alpha=0.05, size=0.01, s=0.01),\n",
    "                      diag_kind=\"kde\", diag_kws=dict(shade=True))\n",
    "\n",
    "fig = pp.fig\n",
    "fig.subplots_adjust(top=0.93, wspace=0.3)\n",
    "t = fig.suptitle('Pairwise Plots', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8qbWLPNO5fg"
   },
   "source": [
    "### KDE для кодов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMOLm6vsO7Kb"
   },
   "outputs": [],
   "source": [
    "def smart_gridsearch(begin, end, data, resolution = 7, rel_x_epsilon = 0.01, rtol = 0.001, n_jobs = 2, cv = 5):\n",
    "    while True:\n",
    "        grid = np.logspace(np.log10(begin), np.log10(end), resolution)\n",
    "        print(\"Поиск по сетке: \", grid)\n",
    "        params = {'bandwidth': grid}\n",
    "        \n",
    "        grid_search = GridSearchCV(KernelDensity(rtol = rtol, kernel='gaussian'), params, n_jobs = n_jobs, verbose = 10, cv = cv)\n",
    "        grid_search.fit(data)\n",
    "        \n",
    "        if grid_search.best_index_ == 0:\n",
    "            begin *= begin / end\n",
    "            end = grid[1]\n",
    "        elif grid_search.best_index_ == resolution - 1:\n",
    "            end *= end / grid[-2]\n",
    "            begin = grid[-2]\n",
    "        else:\n",
    "            begin = grid[grid_search.best_index_ - 1]\n",
    "            end = grid[grid_search.best_index_ + 1]\n",
    "\n",
    "            if end - begin < rel_x_epsilon * grid[grid_search.best_index_]:\n",
    "                return grid_search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVCcZMpjPII5",
    "outputId": "3648b7da-ec10-446c-8ff0-1dee7f369aa9"
   },
   "outputs": [],
   "source": [
    "KDE_codes_X = smart_gridsearch(0.3, 0.6, codes_pca_X, n_jobs = n_jobs).best_estimator_\n",
    "KDE_codes_X.set_params(rtol = 0.0)\n",
    "print(KDE_codes_X.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KDE_codes_L = smart_gridsearch(0.01, 0.2, codes_pca_L, n_jobs = n_jobs).best_estimator_\n",
    "KDE_codes_L.set_params(rtol = 0.0)\n",
    "print(KDE_codes_L.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KDE_codes_X_L = smart_gridsearch(0.3, 0.6, codes_pca_X_L, n_jobs = n_jobs).best_estimator_\n",
    "KDE_codes_X_L.set_params(rtol = 0.0)\n",
    "#KDE_codes_12 = KernelDensity(rtol = 0.0, bandwidth = max(KDE_codes_1.get_params()['bandwidth'], KDE_codes_2.get_params()['bandwidth']))\n",
    "print(KDE_codes_X_L.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90JoA1fZPLM8"
   },
   "outputs": [],
   "source": [
    "info['bandwidth_X'] = KDE_codes_X.get_params()['bandwidth']\n",
    "info['bandwidth_L'] = KDE_codes_L.get_params()['bandwidth']\n",
    "info['bandwidth_X_L'] = KDE_codes_X_L.get_params()['bandwidth']\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtQMUyL7PTvt"
   },
   "source": [
    "## Подсчёт взаимной информации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ I(X, L) $\n",
    "\n",
    "Взаимная информация между входом и слоем:\n",
    "\n",
    "$$\n",
    "I(X, L) = H(X) + H(L) - H(X,L)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hFnw6XaPXnb"
   },
   "outputs": [],
   "source": [
    "def _loo_step(bandwidth, samples, i):\n",
    "    loo_samples = samples\n",
    "    np.delete(loo_samples, i)\n",
    "    \n",
    "    kde = KernelDensity(bandwidth=bandwidth, kernel='gaussian')\n",
    "    kde.fit(loo_samples)\n",
    "    return kde.score_samples([samples[i]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB5i2VmjPcub"
   },
   "outputs": [],
   "source": [
    "def entropy_leave_one_out_parallel(path, bandwidth, samples, n_jobs = 2, first_N = None, parts = 10, recover_saved = False):\n",
    "    \"\"\"\n",
    "    Параллельное вычисление оценки энтропии методом убрать-один-элемент.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Создание временных папок для сохранения прогресса.\n",
    "    parts_path = path + \"LOO_PARTS/\"\n",
    "    os.makedirs(parts_path, exist_ok=True)\n",
    "\n",
    "    # Если дано first_N, энтропия будет оцениваться только на первых first_N элементах.\n",
    "    N = 0\n",
    "    if first_N is None:\n",
    "        N = len(samples)\n",
    "    else:\n",
    "        N = first_N\n",
    "\n",
    "    # Число частей и массив, их содержащий.\n",
    "    N_per_part = N // parts\n",
    "    log_probs = []\n",
    "\n",
    "    # Восстанавливаем прогресс, если требуется.\n",
    "    recovered_parts = 0\n",
    "    if recover_saved:\n",
    "        for filename in os.listdir(parts_path):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                log_probs.append(np.loadtxt(parts_path + filename))\n",
    "                recovered_parts += 1\n",
    "\n",
    "    print(\"Восстановлено блоков данных: %d\" % recovered_parts)\n",
    "\n",
    "    # Подсчёт логарифма вероятности в точках.\n",
    "    for part in range(recovered_parts, parts):\n",
    "        log_probs.append(\n",
    "            np.array(\n",
    "                Parallel(n_jobs = n_jobs, verbose = 10, batch_size = 8)(\n",
    "                    delayed(_loo_step)(bandwidth, samples, i) for i in range(part * N_per_part, min((part + 1) * N_per_part, N))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        np.savetxt(parts_path + str(part) + \".csv\", log_probs[part], delimiter=\"\\n\")\n",
    "    \n",
    "    # Объединение в один массив.\n",
    "    log_prob = np.concatenate(log_probs)\n",
    "\n",
    "    # Суммирование и нахождение стандартного отклонения.\n",
    "    average = -math.fsum(log_prob) / N    \n",
    "    squared_deviations = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        squared_deviations[i] = (log_prob[i] - average)**2\n",
    "    standard_deviation = np.sqrt(math.fsum(squared_deviations) / (N * (N - 1)))\n",
    "    \n",
    "    # Удаление временных файлов.\n",
    "    shutil.rmtree(parts_path)\n",
    "        \n",
    "    return average, standard_deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H(X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvhNZC1IPiVO",
    "outputId": "96dfe559-ee14-4712-ae44-f8e76aac951d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Исходный набор данных.\n",
    "latent_entropy_X, latent_entropy_error_X = entropy_leave_one_out_parallel(full_path,\n",
    "                                                                          KDE_codes_X.get_params()['bandwidth'],\n",
    "                                                                          codes_pca_X,\n",
    "                                                                          n_jobs = n_jobs,\n",
    "                                                                          first_N = 60000, \n",
    "                                                                          recover_saved = False)\n",
    "\n",
    "print(\"LH_X: %f, errLH_X: %f\" % (latent_entropy_X, latent_entropy_error_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H(L)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Слой.\n",
    "latent_entropy_L, latent_entropy_error_L = entropy_leave_one_out_parallel(full_path,\n",
    "                                                                          KDE_codes_L.get_params()['bandwidth'],\n",
    "                                                                          codes_pca_L,\n",
    "                                                                          n_jobs = n_jobs,\n",
    "                                                                          first_N = 60000,\n",
    "                                                                          recover_saved = False)\n",
    "\n",
    "print(\"LH_L: %f, errLH_L: %f\" % (latent_entropy_L, latent_entropy_error_L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H(X,L)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Совместное распределение.\n",
    "latent_entropy_X_L, latent_entropy_error_X_L = entropy_leave_one_out_parallel(full_path,\n",
    "                                                                              KDE_codes_X_L.get_params()['bandwidth'],\n",
    "                                                                              codes_pca_X_L,\n",
    "                                                                              n_jobs = n_jobs,\n",
    "                                                                              first_N = 60000,\n",
    "                                                                              recover_saved = False)\n",
    "\n",
    "print(\"LH_X_L: %f, errLH_X_L: %f\" % (latent_entropy_X_L, latent_entropy_error_X_L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duowAmegPkzY"
   },
   "outputs": [],
   "source": [
    "info['latent_entropy_X'] = latent_entropy_X\n",
    "info['latent_entropy_error_X'] = latent_entropy_error_X\n",
    "\n",
    "info['latent_entropy_L'] = latent_entropy_L\n",
    "info['latent_entropy_error_L'] = latent_entropy_error_L\n",
    "\n",
    "info['latent_entropy_X_L'] = latent_entropy_X_L\n",
    "info['latent_entropy_error_X_L'] = latent_entropy_error_X_L\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBw5gOcqIF00"
   },
   "outputs": [],
   "source": [
    "# Коэффициент растяжения при денормализации.\n",
    "PCA_codes_defc_X = np.abs(np.linalg.det( PCA_codes_X.inverse_transform(np.eye(codes_dim_X)) -\n",
    "                                         PCA_codes_X.inverse_transform(np.zeros((codes_dim_X, codes_dim_X))) ))\n",
    "\n",
    "PCA_codes_defc_L = np.abs(np.linalg.det( PCA_codes_L.inverse_transform(np.eye(codes_dim_L)) -\n",
    "                                         PCA_codes_L.inverse_transform(np.zeros((codes_dim_L, codes_dim_L))) ))\n",
    "                                                                       \n",
    "PCA_codes_defc_X_L = np.abs(np.linalg.det( PCA_codes_X_L.inverse_transform(np.eye(codes_X_layerdim)) -\n",
    "                                          PCA_codes_X_L.inverse_transform(np.zeros((codes_X_layerdim, codes_X_layerdim))) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkjQc-SVIV_Q"
   },
   "outputs": [],
   "source": [
    "# Соответствующая энтропия.\n",
    "PCA_codes_transform_entropy_X = np.log(PCA_codes_defc_X)\n",
    "PCA_codes_transform_entropy_L = np.log(PCA_codes_defc_L)\n",
    "PCA_codes_transform_entropy_X_L = np.log(PCA_codes_defc_X_L)\n",
    "\n",
    "print(\"PCA_TH_X: %f\" % PCA_codes_transform_entropy_X)\n",
    "print(\"PCA_TH_L: %f\" % PCA_codes_transform_entropy_L)\n",
    "print(\"PCA_TH_X_L: %f\" % PCA_codes_transform_entropy_X_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['PCA_codes_transform_entropy_X'] = PCA_codes_transform_entropy_X\n",
    "info['PCA_codes_transform_entropy_L'] = PCA_codes_transform_entropy_L\n",
    "info['PCA_codes_transform_entropy_X_L'] = PCA_codes_transform_entropy_X_L\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzRnFJODT5Kh"
   },
   "outputs": [],
   "source": [
    "# Итоговая оценка энтропии.\n",
    "entropy_X = latent_entropy_X + PCA_codes_transform_entropy_X\n",
    "entropy_error_X = latent_entropy_error_X\n",
    "\n",
    "entropy_L = latent_entropy_L + PCA_codes_transform_entropy_L\n",
    "entropy_error_L = latent_entropy_error_L\n",
    "\n",
    "entropy_X_L = latent_entropy_X_L + PCA_codes_transform_entropy_X_L\n",
    "entropy_error_X_L = latent_entropy_error_X_L\n",
    "\n",
    "print(\"H_X: %f, errH_X: %f\\nH_L: %f, errH_L %f\\nH_X_L: %f, errH_X_L: %f\" %\n",
    "      (entropy_X, entropy_error_X,\n",
    "       entropy_L, entropy_error_L,\n",
    "       entropy_X_L, entropy_error_X_L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_information_X_L = entropy_X + entropy_L - entropy_X_L\n",
    "mutual_information_error_X_L = entropy_error_X + entropy_error_L + entropy_error_X_L\n",
    "\n",
    "print(\"MI: %f, errMI: %f\" % (mutual_information_X_L, mutual_information_error_X_L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['entropy_X'] = entropy_X\n",
    "info['entropy_error_X'] = entropy_error_X\n",
    "\n",
    "info['entropy_L'] = entropy_L\n",
    "info['entropy_error_L'] = entropy_error_L\n",
    "\n",
    "info['entropy_X_L'] = entropy_X_L\n",
    "info['entropy_error_X_L'] = entropy_error_X_L\n",
    "\n",
    "\n",
    "info['mutual_information_X_L'] = mutual_information_X_L\n",
    "info['mutual_information_error_X_L'] = mutual_information_error_X_L\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ I(L, Y) $\n",
    "\n",
    "Взаимная информация между слоем и меткой:\n",
    "\n",
    "$$\n",
    "I(L, Y) = H(L) - H(L \\mid Y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "H(L \\mid Y) = \\sum_{\\text{Im} \\, Y} p_Y(y) \\cdot \\left[ - \\int\\limits_{\\text{Im} \\, Y} \\rho_L(l \\mid y) \\ln \\left( \\rho_L(l \\mid y) \\right) \\, dl \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Распределение меток\n",
    "\n",
    "n_labels = 10\n",
    "P_Y = np.zeros(n_labels)\n",
    "for i in range(ds_train_Y.shape[0]):\n",
    "    P_Y[ds_train_Y[i]] += 1\n",
    "    \n",
    "P_Y /= ds_train_Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_L_mid_Y_array = np.zeros(n_labels)\n",
    "entropy_L_mid_Y_error_array = np.zeros(n_labels)\n",
    "\n",
    "for y in range(n_labels):\n",
    "    print(\"Расчёт для метки %d\" % y)\n",
    "    \n",
    "    # Получение кодов для данной метки.\n",
    "    codes_L_mid_Y = np.array([codes_L[i] for i in range(codes_L.shape[0]) if ds_train_Y[i] == y])\n",
    "    \n",
    "    # PCA\n",
    "    PCA_codes_L_mid_Y = PCA(n_components=codes_dim_L, whiten=True)\n",
    "    codes_pca_L_mid_Y = np.array(PCA_codes_L_mid_Y.fit_transform(codes_L_mid_Y))\n",
    "    \n",
    "    # KDE\n",
    "    KDE_codes_L_mid_Y = smart_gridsearch(0.01, 0.2, codes_pca_L_mid_Y, n_jobs = n_jobs).best_estimator_\n",
    "    KDE_codes_L_mid_Y.set_params(rtol = 0.0)\n",
    "    print(KDE_codes_L_mid_Y.get_params())\n",
    "    \n",
    "    # Совместное распределение.\n",
    "    le_L_mid_X, le_error_L_mid_X = entropy_leave_one_out_parallel(full_path,\n",
    "                                                                  KDE_codes_L_mid_Y.get_params()['bandwidth'],\n",
    "                                                                  codes_pca_X_L,\n",
    "                                                                  n_jobs = n_jobs,\n",
    "                                                                  first_N = 60000,\n",
    "                                                                  recover_saved = False)\n",
    "\n",
    "    print(\"LH_L_mid_Y(%d): %f, errLH_L_mid_Y(%d): %f\" % (y, le_L_mid_X, y, le_error_L_mid_X))\n",
    "    \n",
    "    # Изменение энтропии при денормализации.\n",
    "    PCA_codes_defc_L_mid_Y = np.abs(\n",
    "        np.linalg.det( PCA_codes_L_mid_Y.inverse_transform(np.eye(codes_dim_L)) -\n",
    "        PCA_codes_L_mid_Y.inverse_transform(np.zeros((codes_dim_L, codes_dim_L))) )\n",
    "    )\n",
    "    PCA_codes_transform_entropy_L_mid_Y = np.log(PCA_codes_defc_L_mid_Y)\n",
    "    \n",
    "    # Итоговая энтропия.\n",
    "    entropy_L_mid_Y_array[y] = le_L_mid_X + PCA_codes_transform_entropy_L_mid_Y\n",
    "    entropy_L_mid_Y_error_array[y] = le_error_L_mid_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_L_mid_Y = P_Y @ entropy_L_mid_Y_array\n",
    "entropy_L_mid_Y_error = P_Y @ entropy_L_mid_Y_error_array\n",
    "\n",
    "print(\"H_L_mid_Y: %f, errH_L_mid_Y: %f\" % (entropy_L_mid_Y, entropy_L_mid_Y_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_information_L_Y = entropy_L - entropy_L_mid_Y\n",
    "mutual_information_error_L_Y = entropy_error_L + entropy_L_mid_Y_error\n",
    "\n",
    "print(\"MI: %f, errMI: %f\" % (mutual_information_L_Y, mutual_information_error_L_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['entropy_L_mid_Y'] = entropy_L_mid_Y\n",
    "info['entropy_L_mid_Y_error'] = entropy_L_mid_Y_error\n",
    "\n",
    "\n",
    "info['mutual_information_L_Y'] = mutual_information_L_Y\n",
    "info['mutual_information_error_L_Y'] = mutual_information_error_L_Y\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Information v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
