{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information (synthetic)\n",
    "\n",
    "Эксперименты с оценкой энтропии для синтетических данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PuuYTaasyqC"
   },
   "source": [
    "# Преамбула"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J20_kxWGua1g"
   },
   "source": [
    "## Библиотеки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knCL6YcRtDSI"
   },
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKI49Wt7s1ZH",
    "outputId": "00b0c2c1-a14d-4681-b725-966bd57299dc"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81zMD0EitGlJ"
   },
   "source": [
    "### Math, Numpy, Scipy, Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TrbCI8-s4re"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as sps\n",
    "import scipy.linalg as spl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GwNnxNiDgYw"
   },
   "source": [
    "### Matplotlib, Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0SjY2FyDgiY"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWH6JIAJtbs3"
   },
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rb9G9YxtfD3"
   },
   "outputs": [],
   "source": [
    "# Деревья.\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "# Метрика.\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "# Метод главных компонент.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Выбор модели по кросс-валидации (поиск по сетке).\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLVaeOz9tbwI"
   },
   "source": [
    "### Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5R5GjsuMtPe4"
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "n_jobs = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCTOH5CQuULh"
   },
   "source": [
    "### OS, shutil, Json, CSV, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwC7bZldt4qY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import csv\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9zP5A4nufLs"
   },
   "source": [
    "## Вспомогательное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flPbS2ebuY7h"
   },
   "outputs": [],
   "source": [
    "# Информация об опыте.\n",
    "info = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RyvEh9bulAV"
   },
   "outputs": [],
   "source": [
    "def normalize_uint8(data, label):\n",
    "    \"\"\"Нормализация: `uint8` -> `float32`.\"\"\"\n",
    "    return tf.cast(data, tf.float32) / 255.0, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5xA0W2DvIBq"
   },
   "outputs": [],
   "source": [
    "def imshow_array(array):\n",
    "    \"\"\"Отображение массива нормированных пикселей.\"\"\"\n",
    "    plt.axis('off')\n",
    "    plt.imshow((255.0 * array).astype(np.uint8), cmap=plt.get_cmap(\"gray\"), vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5OjeuhXvP6O"
   },
   "outputs": [],
   "source": [
    "def dataset_Y_to_X(X, Y):\n",
    "    \"\"\"Поменять у датасета пары (X, Y) на (X, X) (нужно, например, для обучения автоэнкодера).\"\"\"\n",
    "    return X, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qo0MXHDowRLZ"
   },
   "outputs": [],
   "source": [
    "def concave_loss(y_true, y_pred):\n",
    "    \"\"\"Вогнутая функция потерь, дающая более четкие изображения при обучении.\"\"\"\n",
    "    delta = tf.keras.backend.abs(y_true - y_pred)\n",
    "    squared = tf.keras.backend.square(y_true - y_pred)\n",
    "    return tf.keras.backend.mean(delta - 0.5 * squared, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9o4czLSwxib"
   },
   "source": [
    "## Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aSdv1fxZwt8N",
    "outputId": "c08303d3-5680-4b43-f4f0-26fa82612a4a"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Путь к папке с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tneJ2JaEwztO"
   },
   "outputs": [],
   "source": [
    "#path = \"/content/drive/My Drive/Information_v2/\"\n",
    "path = os.path.abspath(os.getcwd()) + \"/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxwU2N_pxJP5"
   },
   "source": [
    "# Синтетические данные\n",
    "\n",
    "Для первоначальных экспериментов данные синтезируются путем сэмплирования точек из некоторого распределения с последующим отображением на некоторое многообразие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eDZuNXnxBr-"
   },
   "outputs": [],
   "source": [
    "dataset_dim_1 = 32 # Размерность данных 1.\n",
    "latent_dim_1  = 2  # Реальная (скрытая) размерность данных 1.\n",
    "\n",
    "dataset_dim_2 = 32 # Размерность данных 2.\n",
    "latent_dim_2  = 2  # Реальная (скрытая) размерность данных 2.\n",
    "\n",
    "final_noize_stdev = 0.0 # Стандартное отклонение шума, складываемого с выходом функции.\n",
    "samples_number = 60000 # Размер выборки.\n",
    "tests_number   = 10000 # Размер тестовой выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQK4o5df_Jki"
   },
   "outputs": [],
   "source": [
    "experiments_path = path + \"mutual_information/synthetic/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Miscellaneous.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4D4QTSroJPXR"
   },
   "source": [
    "### Выбор случайной величины и отображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQ_mG9OeCvm1"
   },
   "outputs": [],
   "source": [
    "random_variable_1 = One_Ring(less_rad = 3.0 * np.pi, bigg_rad = 4.0 * np.pi)\n",
    "random_variable_2 = One_Ring(less_rad = 3.0 * np.pi, bigg_rad = 4.0 * np.pi)\n",
    "#rv_id_ensemble([sps.uniform(0.0, 2.0 * np.pi), sps.uniform(0.0, 2.0 * np.pi)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_1 = mapping_ensemble([mapping_segmented([mapping_circle()] * 8, [(np.pi * i, np.pi * (i+1)) for i in range(0,8)]),\n",
    "                              mapping_segmented([mapping_circle()] * 8, [(np.pi * i, np.pi * (i+1)) for i in range(0,8)])])\n",
    "\n",
    "mapping_2 = mapping_ensemble([mapping_segmented([mapping_circle()] * 8, [(np.pi * i, np.pi * (i+1)) for i in range(0,8)]),\n",
    "                              mapping_segmented([mapping_circle()] * 8, [(np.pi * i, np.pi * (i+1)) for i in range(0,8)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWuXJNQ6C_bT"
   },
   "outputs": [],
   "source": [
    "# Проверка входной размерности.\n",
    "assert latent_dim_1 == mapping_1.input_dim\n",
    "assert latent_dim_2 == mapping_2.input_dim\n",
    "\n",
    "true_mutual_information = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nM9z7mipDQND"
   },
   "source": [
    "### Генерация набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hM8IKyNZD4tr"
   },
   "outputs": [],
   "source": [
    "# Матрица поворота и повышения размерности.\n",
    "Q_1 = sps.ortho_group.rvs(dim = dataset_dim_1)\n",
    "#Q_1 = np.eye(dataset_dim)\n",
    "transform_1 = Q_1[:,:mapping_1.output_dim]\n",
    "\n",
    "Q_2 = sps.ortho_group.rvs(dim = dataset_dim_2)\n",
    "#Q_2 = np.eye(dataset_dim)\n",
    "transform_2 = Q_2[:,:mapping_2.output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yn_tuKO73HSp"
   },
   "outputs": [],
   "source": [
    "def get_samples(X, mapping, dataset_dim, transform_matrix, final_noize_stdev = 0.05):\n",
    "    \"\"\"\n",
    "    Генерация набора данных.\n",
    "    \"\"\"\n",
    "\n",
    "    # Данные во внутреннем представлении.\n",
    "    samples_number = X.shape[0]\n",
    "    \n",
    "    # Отображение шума в пространство большей размерности.\n",
    "    Y = np.zeros((samples_number, dataset_dim))\n",
    "    noize = sps.norm(loc=0, scale=final_noize_stdev)\n",
    "    for i in range(samples_number):\n",
    "        Y[i] = transform_matrix @ mapping.map(X[i]) + noize.rvs(dataset_dim)\n",
    "            \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDkJjM2CDTuJ"
   },
   "outputs": [],
   "source": [
    "#X_1 = random_variable_1.rvs(samples_number)\n",
    "#X_2 = X_1#random_variable_2.rvs(samples_number)\n",
    "\n",
    "#T_1 = random_variable_1.rvs(tests_number)\n",
    "#T_2 = T_1#random_variable_2.rvs(tests_number)\n",
    "\n",
    "X_1 = np.concatenate((np.expand_dims(sps.uniform(loc=0.0, scale=8.0 * np.pi).rvs(samples_number), 1), np.expand_dims(sps.uniform(loc=0.0, scale=4.0 * np.pi).rvs(samples_number), 1) ), 1)\n",
    "X_2 = np.concatenate((np.expand_dims(sps.uniform(loc=0.0, scale=4.0 * np.pi).rvs(samples_number), 1), np.expand_dims(sps.uniform(loc=0.0, scale=8.0 * np.pi).rvs(samples_number), 1) ), 1)\n",
    "\n",
    "X_2[:,0] += X_1[:,1]\n",
    "\n",
    "T_1 = np.concatenate((np.expand_dims(sps.uniform(loc=0.0, scale=8.0 * np.pi).rvs(tests_number), 1), np.expand_dims(sps.uniform(loc=0.0, scale=4.0 * np.pi).rvs(tests_number), 1) ), 1)\n",
    "T_2 = np.concatenate((np.expand_dims(sps.uniform(loc=0.0, scale=4.0 * np.pi).rvs(tests_number), 1), np.expand_dims(sps.uniform(loc=0.0, scale=8.0 * np.pi).rvs(tests_number), 1) ), 1)\n",
    "\n",
    "T_2[:,0] += T_1[:,1]\n",
    "\n",
    "samples_1 = get_samples(X_1, mapping_1, dataset_dim_1, transform_1, final_noize_stdev)\n",
    "tests_1   = get_samples(T_1, mapping_1,   dataset_dim_1, transform_1, final_noize_stdev)\n",
    "\n",
    "samples_2 = get_samples(X_2, mapping_2, dataset_dim_2, transform_2, final_noize_stdev)\n",
    "tests_2   = get_samples(T_2, mapping_2,   dataset_dim_2, transform_2, final_noize_stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "J_18kMWsERTp",
    "outputId": "a98fd5ee-c238-4a73-d9fe-0428ba074967"
   },
   "outputs": [],
   "source": [
    "projected = np.array([samples_1[i][0:8] for i in range(1000)])\n",
    "\n",
    "draw_pair_plot = True\n",
    "if draw_pair_plot:\n",
    "    pp = sns.pairplot(pd.DataFrame(projected), height = 2.0, aspect=1.6,\n",
    "                      plot_kws=dict(edgecolor=\"k\", linewidth=0.0, alpha=0.1, size=0.01, s=0.01),\n",
    "                      diag_kind=\"kde\", diag_kws=dict(shade=True))\n",
    "\n",
    "    fig = pp.fig\n",
    "    fig.subplots_adjust(top=0.93, wspace=0.3)\n",
    "    t = fig.suptitle('Pairwise Plots', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pRyKwjKCqml"
   },
   "source": [
    "### Путь к результатам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBi5v6hSAk1K"
   },
   "outputs": [],
   "source": [
    "dataset_path = experiments_path + (\"%.3e\" % final_noize_stdev) + \"/\" + str(samples_number) + \"_\" + str(tests_number) + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R73vj8c3x3-Y"
   },
   "source": [
    "# Оценка энтропии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2coUXxYyAVx"
   },
   "source": [
    "## Автокодировщик\n",
    "\n",
    "Сжатие данных предлагается делать автокодировщиком.\n",
    "Для архитектуры специфицируется только формат входных данных, а также размерность внутреннего представления (кодов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aa1TECWvxuEF"
   },
   "outputs": [],
   "source": [
    "# РАЗМЕРНОСТЬ КОДА.\n",
    "# #\n",
    "# #\n",
    " \n",
    "codes_dim_1 = 2\n",
    "codes_dim_2 = 2\n",
    "\n",
    "# #\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3McJCha_b_G"
   },
   "outputs": [],
   "source": [
    "# Число эпох для обучения.\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOu7nB9gx9vH"
   },
   "outputs": [],
   "source": [
    "full_path = dataset_path + \"autoencoders/\"\n",
    "os.makedirs(full_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lwIjSKL_gMv"
   },
   "outputs": [],
   "source": [
    "info['dataset_dim_1'] = dataset_dim_1\n",
    "info['latent_dim_1'] = latent_dim_1\n",
    "\n",
    "info['dataset_dim_2'] = dataset_dim_2\n",
    "info['latent_dim_2'] = latent_dim_2\n",
    "\n",
    "info['samples_number'] = samples_number\n",
    "info['tests_number'] = tests_number\n",
    "\n",
    "info['codes_dim_1'] = codes_dim_1\n",
    "info['codes_dim_2'] = codes_dim_2\n",
    "info['epochs'] = epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45W1qKkoB90y"
   },
   "source": [
    "### Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2475m-kB1mm"
   },
   "outputs": [],
   "source": [
    "def dense_autoencoder(shape_input, dimension):\n",
    "    # Инициализация весов.\n",
    "    init = tf.keras.initializers.RandomNormal(stddev = 0.02)\n",
    "\n",
    "    # Входные данные генератора / выборки.\n",
    "    input_layer = tf.keras.layers.Input(shape_input)\n",
    "    next_layer = input_layer\n",
    "    next_layer = tf.keras.layers.GaussianNoise(0.02)(next_layer)\n",
    "\n",
    "    # 1 блок слоёв.\n",
    "    next_layer = tfa.layers.SpectralNormalization(tf.keras.layers.Dense(64, kernel_initializer = init),\n",
    "                                                  power_iterations = 8)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "\n",
    "    # 2 блок слоёв.\n",
    "    next_layer = tfa.layers.SpectralNormalization(tf.keras.layers.Dense(32, kernel_initializer = init),\n",
    "                                                  power_iterations = 8)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    \n",
    "    # 3 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.Dense(8, kernel_initializer = init)(next_layer)\n",
    "    #next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    \n",
    "    # Бутылочное горлышко.\n",
    "    next_layer = tfa.layers.SpectralNormalization(tf.keras.layers.Dense(dimension),\n",
    "                                                  power_iterations = 8)(next_layer)\n",
    "    bottleneck = tf.keras.layers.Activation('tanh', name='bottleneck')(next_layer)\n",
    "\n",
    "    # Модель кодировщика.\n",
    "    encoder = tf.keras.Model(input_layer, bottleneck)\n",
    "\n",
    "    # Начало модели декодировщика.\n",
    "    input_code_layer = tf.keras.layers.Input((dimension))\n",
    "    next_layer = input_code_layer\n",
    "\n",
    "    # 3 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.Dense(8, kernel_initializer = init)(next_layer)\n",
    "    #next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    \n",
    "    # 2 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Dense(32, kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "\n",
    "    # 1 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Dense(64, kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    \n",
    "    # 0 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Dense(shape_input[0])(next_layer) # Подразумевается, что вход - всё равно вектор.\n",
    "    #next_layer = tf.keras.layers.Activation('tanh')(next_layer)\n",
    "    \n",
    "    output_layer = next_layer\n",
    "    \n",
    "    # Модель.\n",
    "    decoder = tf.keras.models.Model(input_code_layer, output_layer) # Декодировщик.\n",
    "    autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "\n",
    "    # Компиляция модели.\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 5e-3)\n",
    "    autoencoder.compile(loss = 'mse', optimizer = opt, loss_weights = [1.0])\n",
    "    \n",
    "    return encoder, decoder, autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzOjEmMjCCWS"
   },
   "source": [
    "### Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmzV6titB6zT"
   },
   "outputs": [],
   "source": [
    "#encoder = tf.keras.models.load_model(full_path + \"encoder.h5\")\n",
    "#decoder = tf.keras.models.load_model(full_path + \"decoder.h5\")\n",
    "#autoencoder = autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "#autoencoder.compile(loss = 'mse', optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3), loss_weights = [1.0])\n",
    "\n",
    "#with open(full_path + 'info.json', 'r') as fp:\n",
    "#    info = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ld3eyuWPCHtW"
   },
   "outputs": [],
   "source": [
    "encoder_1, decoder_1, autoencoder_1 = dense_autoencoder((dataset_dim_1,), codes_dim_1)\n",
    "encoder_2, decoder_2, autoencoder_2 = dense_autoencoder((dataset_dim_2,), codes_dim_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_1.fit(samples_1, samples_1, epochs=epochs, validation_data=(tests_1, tests_1), batch_size=samples_number // 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_1.compile(loss = 'mse', optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3), loss_weights = [1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder_1.fit(samples_1, samples_1, epochs=1000, validation_data=(tests_1, tests_1), batch_size=samples_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_2.fit(samples_2, samples_2, epochs=epochs, validation_data=(tests_2, tests_2), batch_size=samples_number // 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_2.compile(loss = 'mse', optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3), loss_weights = [1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder_2.fit(samples_2, samples_2, epochs=1000, validation_data=(tests_2, tests_2), batch_size=samples_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQxF6KcgNag-",
    "outputId": "21f60548-9dd3-4430-a27e-3e871a431cfe"
   },
   "outputs": [],
   "source": [
    "# Сохранение моделей.\n",
    "autoencoder_1.save(full_path + \"autoencoder_1.h5\")\n",
    "encoder_1.save(full_path + \"encoder_1.h5\")\n",
    "decoder_1.save(full_path + \"decoder_1.h5\")\n",
    "\n",
    "autoencoder_2.save(full_path + \"autoencoder_2.h5\")\n",
    "encoder_2.save(full_path + \"encoder_2.h5\")\n",
    "decoder_2.save(full_path + \"decoder_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuImkbabNcqj"
   },
   "outputs": [],
   "source": [
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPMRQmWmNjIS"
   },
   "source": [
    "### Получение кодов всех элементов набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mksJnyPiNkhD"
   },
   "outputs": [],
   "source": [
    "codes_1 = np.array(encoder_1.predict(samples_1))\n",
    "codes_2 = np.array(encoder_2.predict(samples_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_12 = np.concatenate((codes_1, codes_2), 1)\n",
    "codes_dim_12 = codes_dim_1 + codes_dim_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EMu1RqxN7vj"
   },
   "outputs": [],
   "source": [
    "codes_pca_dim_1 = codes_dim_1\n",
    "PCA_codes_1 = PCA(n_components=codes_pca_dim_1, whiten=True)\n",
    "codes_pca_1 = np.array(PCA_codes_1.fit_transform(codes_1))\n",
    "\n",
    "codes_pca_dim_2 = codes_dim_2\n",
    "PCA_codes_2 = PCA(n_components=codes_pca_dim_2, whiten=True)\n",
    "codes_pca_2 = np.array(PCA_codes_2.fit_transform(codes_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_pca_dim_12 = codes_dim_12\n",
    "PCA_codes_12 = PCA(n_components=codes_pca_dim_12, whiten=True)\n",
    "codes_pca_12 = np.array(PCA_codes_12.fit_transform(codes_12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = sns.pairplot(pd.DataFrame(codes_pca_12[0:10000]), height = 2.0, aspect=1.6,\n",
    "                      plot_kws=dict(edgecolor=\"k\", linewidth=0.0, alpha=0.05, size=0.01, s=0.01),\n",
    "                      diag_kind=\"kde\", diag_kws=dict(shade=True))\n",
    "\n",
    "fig = pp.fig\n",
    "fig.subplots_adjust(top=0.93, wspace=0.3)\n",
    "t = fig.suptitle('Pairwise Plots', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8qbWLPNO5fg"
   },
   "source": [
    "### KDE для кодов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYPLhT5mO58e"
   },
   "outputs": [],
   "source": [
    "# Загрузка параметров KDE.\n",
    "\n",
    "#with open(full_path + 'info.json', 'r') as fp:\n",
    "#    info = json.load(fp)\n",
    "\n",
    "#kde_codes = KernelDensity(bandwidth=info['bandwidth'], kernel='gaussian')\n",
    "#kde_codes.fit(codes_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMOLm6vsO7Kb"
   },
   "outputs": [],
   "source": [
    "def smart_gridsearch(begin, end, data, resolution = 7, rel_x_epsilon = 0.01, rtol = 0.001, n_jobs = 2, cv = 20):\n",
    "    while True:\n",
    "        grid = np.logspace(np.log10(begin), np.log10(end), resolution)\n",
    "        print(\"Поиск по сетке: \", grid)\n",
    "        params = {'bandwidth': grid}\n",
    "        \n",
    "        grid_search = GridSearchCV(KernelDensity(rtol = rtol, kernel='gaussian'), params, n_jobs = n_jobs, verbose = 10, cv = cv)\n",
    "        grid_search.fit(data)\n",
    "        \n",
    "        if grid_search.best_index_ == 0:\n",
    "            begin *= begin / end\n",
    "            end = grid[1]\n",
    "        elif grid_search.best_index_ == resolution - 1:\n",
    "            end *= end / begin\n",
    "            begin = grid[-2]\n",
    "        else:\n",
    "            begin = grid[grid_search.best_index_ - 1]\n",
    "            end = grid[grid_search.best_index_ + 1]\n",
    "\n",
    "            if end - begin < rel_x_epsilon * grid[grid_search.best_index_]:\n",
    "                return grid_search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVCcZMpjPII5",
    "outputId": "3648b7da-ec10-446c-8ff0-1dee7f369aa9"
   },
   "outputs": [],
   "source": [
    "KDE_codes_1 = smart_gridsearch(0.05, 0.2, codes_pca_1, n_jobs = n_jobs).best_estimator_\n",
    "KDE_codes_1.set_params(rtol = 0.0)\n",
    "print(KDE_codes_1.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KDE_codes_2 = smart_gridsearch(0.05, 0.2, codes_pca_2, n_jobs = n_jobs).best_estimator_\n",
    "KDE_codes_2.set_params(rtol = 0.0)\n",
    "print(KDE_codes_2.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KDE_codes_12 = smart_gridsearch(0.05, 0.2, codes_pca_12, n_jobs = n_jobs).best_estimator_\n",
    "KDE_codes_12.set_params(rtol = 0.0)\n",
    "#KDE_codes_12 = KernelDensity(rtol = 0.0, bandwidth = max(KDE_codes_1.get_params()['bandwidth'], KDE_codes_2.get_params()['bandwidth']))\n",
    "print(KDE_codes_12.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90JoA1fZPLM8"
   },
   "outputs": [],
   "source": [
    "info['bandwidth_1'] = KDE_codes_1.get_params()['bandwidth']\n",
    "info['bandwidth_2'] = KDE_codes_2.get_params()['bandwidth']\n",
    "info['bandwidth_12'] = KDE_codes_12.get_params()['bandwidth']\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtQMUyL7PTvt"
   },
   "source": [
    "## Подсчёт энтропии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hFnw6XaPXnb"
   },
   "outputs": [],
   "source": [
    "def _loo_step(bandwidth, samples, i):\n",
    "    loo_samples = samples\n",
    "    np.delete(loo_samples, i)\n",
    "    \n",
    "    kde = KernelDensity(bandwidth=bandwidth, kernel='gaussian')\n",
    "    kde.fit(loo_samples)\n",
    "    return kde.score_samples([samples[i]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB5i2VmjPcub"
   },
   "outputs": [],
   "source": [
    "def entropy_leave_one_out_parallel(path, bandwidth, samples, n_jobs = 2, first_N = None, parts = 10, recover_saved = False):\n",
    "    \"\"\"\n",
    "    Параллельное вычисление оценки энтропии методом убрать-один-элемент.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Создание временных папок для сохранения прогресса.\n",
    "    parts_path = path + \"LOO_PARTS/\"\n",
    "    os.makedirs(parts_path, exist_ok=True)\n",
    "\n",
    "    # Если дано first_N, энтропия будет оцениваться только на первых first_N элементах.\n",
    "    N = 0\n",
    "    if first_N is None:\n",
    "        N = len(samples)\n",
    "    else:\n",
    "        N = first_N\n",
    "\n",
    "    # Число частей и массив, их содержащий.\n",
    "    N_per_part = N // parts\n",
    "    log_probs = []\n",
    "\n",
    "    # Восстанавливаем прогресс, если требуется.\n",
    "    recovered_parts = 0\n",
    "    if recover_saved:\n",
    "        for filename in os.listdir(parts_path):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                log_probs.append(np.loadtxt(parts_path + filename))\n",
    "                recovered_parts += 1\n",
    "\n",
    "    print(\"Восстановлено блоков данных: %d\" % recovered_parts)\n",
    "\n",
    "    # Подсчёт логарифма вероятности в точках.\n",
    "    for part in range(recovered_parts, parts):\n",
    "        log_probs.append(\n",
    "            np.array(\n",
    "                Parallel(n_jobs = n_jobs, verbose = 10, batch_size = 8)(\n",
    "                    delayed(_loo_step)(bandwidth, samples, i) for i in range(part * N_per_part, min((part + 1) * N_per_part, N))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        np.savetxt(parts_path + str(part) + \".csv\", log_probs[part], delimiter=\"\\n\")\n",
    "    \n",
    "    # Объединение в один массив.\n",
    "    log_prob = np.concatenate(log_probs)\n",
    "\n",
    "    # Суммирование и нахождение стандартного отклонения.\n",
    "    average = -math.fsum(log_prob) / N    \n",
    "    squared_deviations = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        squared_deviations[i] = (log_prob[i] - average)**2\n",
    "    standard_deviation = np.sqrt(math.fsum(squared_deviations) / (N * (N - 1)))\n",
    "    \n",
    "    # Удаление временных файлов.\n",
    "    shutil.rmtree(parts_path)\n",
    "        \n",
    "    return average, standard_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvhNZC1IPiVO",
    "outputId": "96dfe559-ee14-4712-ae44-f8e76aac951d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_entropy_1, latent_entropy_error_1 = entropy_leave_one_out_parallel(full_path, KDE_codes_1.get_params()['bandwidth'], codes_pca_1, n_jobs = n_jobs, recover_saved = False)\n",
    "print(\"LH1: %f, errLH1: %f\" % (latent_entropy_1, latent_entropy_error_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_entropy_2, latent_entropy_error_2 = entropy_leave_one_out_parallel(full_path, KDE_codes_2.get_params()['bandwidth'], codes_pca_2, n_jobs = n_jobs, recover_saved = False)\n",
    "print(\"LH2: %f, errLH2: %f\" % (latent_entropy_2, latent_entropy_error_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_entropy_12, latent_entropy_error_12 = entropy_leave_one_out_parallel(full_path, KDE_codes_12.get_params()['bandwidth'], codes_pca_12, n_jobs = n_jobs, recover_saved = False)\n",
    "print(\"LH12: %f, errLH12: %f\" % (latent_entropy_12, latent_entropy_error_12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duowAmegPkzY"
   },
   "outputs": [],
   "source": [
    "info['latent_entropy_1'] = latent_entropy_1\n",
    "info['latent_entropy_error_1'] = latent_entropy_error_1\n",
    "\n",
    "info['latent_entropy_2'] = latent_entropy_2\n",
    "info['latent_entropy_error_2'] = latent_entropy_error_2\n",
    "\n",
    "info['latent_entropy_12'] = latent_entropy_12\n",
    "info['latent_entropy_error_12'] = latent_entropy_error_12\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBw5gOcqIF00"
   },
   "outputs": [],
   "source": [
    "# Коэффициент растяжения при денормализации.\n",
    "PCA_codes_defc_1 = np.abs(np.linalg.det( PCA_codes_1.inverse_transform(np.eye(codes_pca_dim_1)) -\n",
    "                                         PCA_codes_1.inverse_transform(np.zeros((codes_pca_dim_1, codes_pca_dim_1))) ))\n",
    "\n",
    "PCA_codes_defc_2 = np.abs(np.linalg.det( PCA_codes_2.inverse_transform(np.eye(codes_pca_dim_2)) -\n",
    "                                         PCA_codes_2.inverse_transform(np.zeros((codes_pca_dim_2, codes_pca_dim_2))) ))\n",
    "                                                                       \n",
    "PCA_codes_defc_12 = np.abs(np.linalg.det( PCA_codes_12.inverse_transform(np.eye(codes_pca_dim_12)) -\n",
    "                                          PCA_codes_12.inverse_transform(np.zeros((codes_pca_dim_12, codes_pca_dim_12))) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkjQc-SVIV_Q"
   },
   "outputs": [],
   "source": [
    "# Соответствующая энтропия.\n",
    "PCA_codes_transform_entropy_1 = np.log(PCA_codes_defc_1)\n",
    "PCA_codes_transform_entropy_2 = np.log(PCA_codes_defc_2)\n",
    "PCA_codes_transform_entropy_12 = np.log(PCA_codes_defc_12)\n",
    "\n",
    "print(\"PCA_TH1: %f\" % (PCA_codes_transform_entropy_1))\n",
    "print(\"PCA_TH2: %f\" % (PCA_codes_transform_entropy_2))\n",
    "print(\"PCA_TH12: %f\" % (PCA_codes_transform_entropy_12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['PCA_codes_transform_entropy_1'] = PCA_codes_transform_entropy_1\n",
    "info['PCA_codes_transform_entropy_2'] = PCA_codes_transform_entropy_2\n",
    "info['PCA_codes_transform_entropy_12'] = PCA_codes_transform_entropy_12\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzRnFJODT5Kh"
   },
   "outputs": [],
   "source": [
    "# Итоговая оценка энтропии.\n",
    "entropy_1 = latent_entropy_1 + PCA_codes_transform_entropy_1\n",
    "entropy_error_1 = latent_entropy_error_1\n",
    "\n",
    "entropy_2 = latent_entropy_1 + PCA_codes_transform_entropy_2\n",
    "entropy_error_2 = latent_entropy_error_2\n",
    "\n",
    "entropy_12 = latent_entropy_12 + PCA_codes_transform_entropy_12\n",
    "entropy_error_12 = latent_entropy_error_12\n",
    "\n",
    "print(\"H1: %f, errH1: %f\\nH2: %f, errH2: %f\\nH12: %f, errH12: %f\" %\n",
    "      (entropy_1, entropy_error_1, entropy_2, entropy_error_2, entropy_12, entropy_error_12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_information = entropy_1 + entropy_2 - entropy_12\n",
    "mutual_information_error = entropy_error_12 + entropy_error_1 + entropy_error_2\n",
    "\n",
    "print(\"MI: %f, errMI: %f\" % (mutual_information, mutual_information_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['entropy_1'] = entropy_1\n",
    "info['entropy_error_1'] = entropy_error_1\n",
    "\n",
    "info['entropy_2'] = entropy_2\n",
    "info['entropy_error_2'] = entropy_error_2\n",
    "\n",
    "info['entropy_12'] = entropy_12\n",
    "info['entropy_error_12'] = entropy_error_12\n",
    "\n",
    "\n",
    "info['mutual_information'] = mutual_information\n",
    "info['mutual_information_error'] = mutual_information_error\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Information v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
