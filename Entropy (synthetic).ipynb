{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy (synthetic)\n",
    "\n",
    "Эксперименты с оценкой энтропии для синтетических данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PuuYTaasyqC"
   },
   "source": [
    "# Преамбула"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J20_kxWGua1g"
   },
   "source": [
    "## Библиотеки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knCL6YcRtDSI"
   },
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKI49Wt7s1ZH",
    "outputId": "00b0c2c1-a14d-4681-b725-966bd57299dc"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81zMD0EitGlJ"
   },
   "source": [
    "### Math, Numpy, Scipy, Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TrbCI8-s4re"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as sps\n",
    "import scipy.linalg as spl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GwNnxNiDgYw"
   },
   "source": [
    "### Matplotlib, Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0SjY2FyDgiY"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWH6JIAJtbs3"
   },
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rb9G9YxtfD3"
   },
   "outputs": [],
   "source": [
    "# Деревья.\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "# Метрика.\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "# Метод главных компонент.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Выбор модели по кросс-валидации (поиск по сетке).\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLVaeOz9tbwI"
   },
   "source": [
    "### Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5R5GjsuMtPe4"
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "n_jobs = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCTOH5CQuULh"
   },
   "source": [
    "### OS, shutil, Json, CSV, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwC7bZldt4qY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import csv\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9zP5A4nufLs"
   },
   "source": [
    "## Вспомогательное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flPbS2ebuY7h"
   },
   "outputs": [],
   "source": [
    "# Информация об опыте.\n",
    "info = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RyvEh9bulAV"
   },
   "outputs": [],
   "source": [
    "def normalize_uint8(data, label):\n",
    "    \"\"\"Нормализация: `uint8` -> `float32`.\"\"\"\n",
    "    return tf.cast(data, tf.float32) / 255.0, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5xA0W2DvIBq"
   },
   "outputs": [],
   "source": [
    "def imshow_array(array):\n",
    "    \"\"\"Отображение массива нормированных пикселей.\"\"\"\n",
    "    plt.axis('off')\n",
    "    plt.imshow((255.0 * array).astype(np.uint8), cmap=plt.get_cmap(\"gray\"), vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5OjeuhXvP6O"
   },
   "outputs": [],
   "source": [
    "def dataset_Y_to_X(X, Y):\n",
    "    \"\"\"Поменять у датасета пары (X, Y) на (X, X) (нужно, например, для обучения автоэнкодера).\"\"\"\n",
    "    return X, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qo0MXHDowRLZ"
   },
   "outputs": [],
   "source": [
    "def concave_loss(y_true, y_pred):\n",
    "    \"\"\"Вогнутая функция потерь, дающая более четкие изображения при обучении.\"\"\"\n",
    "    delta = tf.keras.backend.abs(y_true - y_pred)\n",
    "    squared = tf.keras.backend.square(y_true - y_pred)\n",
    "    return tf.keras.backend.mean(delta - 0.5 * squared, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9o4czLSwxib"
   },
   "source": [
    "## Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aSdv1fxZwt8N",
    "outputId": "c08303d3-5680-4b43-f4f0-26fa82612a4a"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Путь к папке с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tneJ2JaEwztO"
   },
   "outputs": [],
   "source": [
    "#path = \"/content/drive/My Drive/Information_v2/\"\n",
    "path = os.path.abspath(os.getcwd()) + \"/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxwU2N_pxJP5"
   },
   "source": [
    "# Синтетические данные\n",
    "\n",
    "Для первоначальных экспериментов данные синтезируются путем сэмплирования точек из некоторого распределения с последующим отображением на некоторое многообразие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eDZuNXnxBr-"
   },
   "outputs": [],
   "source": [
    "dataset_dim = 64  # Размерность данных.\n",
    "latent_dim  = 4  # Реальная (скрытая) размерность данных.\n",
    "final_noize_stdev = 0.05 # Стандартное отклонение шума, складываемого с выходом функции.\n",
    "samples_number = 10000 # Размер выборки.\n",
    "tests_number   = 10000 # Размер тестовой выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQK4o5df_Jki"
   },
   "outputs": [],
   "source": [
    "experiments_path = path + \"entropy/synthetic/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Miscellaneous.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4D4QTSroJPXR"
   },
   "source": [
    "### Выбор случайной величины и отображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQ_mG9OeCvm1"
   },
   "outputs": [],
   "source": [
    "random_variables = [\n",
    "    rv_id_ensemble([sps.uniform(0.0, 8.0 * np.pi),\n",
    "                    sps.uniform(0.0, 8.0 * np.pi),\n",
    "                    sps.uniform(0.0, 8.0 * np.pi),\n",
    "                    sps.uniform(0.0, 8.0 * np.pi)])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = [\n",
    "    mapping_ensemble([mapping_segmented([mapping_circle()] * 8, [(np.pi * i, np.pi * (i+1)) for i in range(8)]),\n",
    "                      mapping_segmented([mapping_circle()] * 8, [(np.pi * i, np.pi * (i+1)) for i in range(8)]),\n",
    "                      mapping_segmented([mapping_circle()] * 8, [(np.pi * i, np.pi * (i+1)) for i in range(8)]),\n",
    "                      mapping_segmented([mapping_circle()] * 8, [(np.pi * i, np.pi * (i+1)) for i in range(8)])]),\n",
    "    \n",
    "    mapping_ensemble([mapping_segmented([mapping_circle()] * 8, [(np.pi * i, np.pi * (i+1)) for i in range(8)])] * 4),\n",
    "    \n",
    "    mapping_ensemble([mapping_segmented([mapping_circle()] * 8, [(np.pi * i, np.pi * (i+1)) for i in range(8)]),\n",
    "                      mapping_segmented([mapping_cycloid(2)] * 8, [(np.pi * i, np.pi * (i+1)) for i in range(8)]),\n",
    "                      mapping_segmented([mapping_epicycloid(2,5)] * 8, [(np.pi * i, np.pi * (i+1)) for i in range(8)]),\n",
    "                      mapping_segmented([mapping_hypocycloid(2,6)] * 8, [(np.pi * i, np.pi * (i+1)) for i in range(8)])]),\n",
    "    \n",
    "    mapping_ensemble([mapping_segmented([mapping_epicycloid(3,5),\n",
    "                                         mapping_hypocycloid(2,6),\n",
    "                                         mapping_epicycloid(1,1),\n",
    "                                         mapping_cycloid(1),\n",
    "                                         mapping_epicycloid(2,2),\n",
    "                                         mapping_hypocycloid(3,9),\n",
    "                                         mapping_hypocycloid(1,8),\n",
    "                                         mapping_cycloid(2)], [(np.pi * i, np.pi * (i+1)) for i in range(8)])] * 4),    \n",
    "    \n",
    "    mapping_ensemble([mapping_segmented([mapping_epicycloid(2,5),\n",
    "                                         mapping_hypocycloid(3,6),\n",
    "                                         mapping_epicycloid(1,16),\n",
    "                                         mapping_cycloid(2),\n",
    "                                         mapping_epicycloid(2,22),\n",
    "                                         mapping_hypocycloid(1,29),\n",
    "                                         mapping_hypocycloid(1,8),\n",
    "                                         mapping_cycloid(3)], [(np.pi * i, np.pi * (i+1)) for i in range(8)])] * 4)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x68Z9AwpAqE5"
   },
   "outputs": [],
   "source": [
    "# НОМЕР ИСХОДНОГО РАСПРЕДЕЛЕНИЯ.\n",
    "# #\n",
    "# #\n",
    "random_variable_index = 1\n",
    "# #\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fj1GnkqWAbma"
   },
   "outputs": [],
   "source": [
    "# НОМЕР ФУНКЦИИ, ЗАДАЮЩЕЙ МНОГООБРАЗИЕ.\n",
    "# #\n",
    "# #\n",
    "mapping_index = 5\n",
    "# #\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWuXJNQ6C_bT"
   },
   "outputs": [],
   "source": [
    "random_variable = random_variables[random_variable_index - 1]\n",
    "mapping = mappings[mapping_index - 1]\n",
    "\n",
    "# Проверка входной размерности.\n",
    "assert latent_dim == mapping.input_dim\n",
    "\n",
    "true_entropy = random_variable.entropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nM9z7mipDQND"
   },
   "source": [
    "### Генерация набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hM8IKyNZD4tr"
   },
   "outputs": [],
   "source": [
    "# Матрица поворота и повышения размерности.\n",
    "Q = sps.ortho_group.rvs(dim = dataset_dim)\n",
    "#Q = np.eye(dataset_dim)\n",
    "transform = Q[:,:mapping.output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yn_tuKO73HSp"
   },
   "outputs": [],
   "source": [
    "def get_samples(random_variable, mapping, samples_number, dataset_dim, transform_matrix, final_noize_stdev = 0.05):\n",
    "    \"\"\"\n",
    "    Генерация набора данных.\n",
    "    \"\"\"\n",
    "\n",
    "    # Данные во внутреннем представлении.\n",
    "    #np.random.seed(42)\n",
    "    X = random_variable.rvs(samples_number)\n",
    "    \n",
    "    # Отображение шума в пространство большей размерности.\n",
    "    Y = np.zeros((samples_number, dataset_dim))\n",
    "    noize = sps.norm(loc=0, scale=final_noize_stdev)\n",
    "    for i in range(samples_number):\n",
    "        Y[i] = transform_matrix @ mapping.map(X[i]) + noize.rvs(dataset_dim)\n",
    "            \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDkJjM2CDTuJ"
   },
   "outputs": [],
   "source": [
    "samples = get_samples(random_variable, mapping, samples_number, dataset_dim, transform, final_noize_stdev)\n",
    "tests   = get_samples(random_variable, mapping, tests_number,   dataset_dim, transform, final_noize_stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "J_18kMWsERTp",
    "outputId": "a98fd5ee-c238-4a73-d9fe-0428ba074967"
   },
   "outputs": [],
   "source": [
    "projected = np.array([samples[i][0:8] for i in range(1000)])\n",
    "\n",
    "draw_pair_plot = True\n",
    "if draw_pair_plot:\n",
    "    pp = sns.pairplot(pd.DataFrame(projected), height = 2.0, aspect=1.6,\n",
    "                      plot_kws=dict(edgecolor=\"k\", linewidth=0.0, alpha=0.1, size=0.01, s=0.01),\n",
    "                      diag_kind=\"kde\", diag_kws=dict(shade=True))\n",
    "\n",
    "    fig = pp.fig\n",
    "    fig.subplots_adjust(top=0.93, wspace=0.3)\n",
    "    t = fig.suptitle('Pairwise Plots', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pRyKwjKCqml"
   },
   "source": [
    "### Путь к результатам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBi5v6hSAk1K"
   },
   "outputs": [],
   "source": [
    "dataset_path = experiments_path + \"rv_\" + str(random_variable_index) + \"_map_\" + str(mapping_index) + \"/\" + str(latent_dim) + \"_\" + str(dataset_dim) + '/' + (\"%.3e\" % final_noize_stdev) + \"/\" + str(samples_number) + \"_\" + str(tests_number) + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R73vj8c3x3-Y"
   },
   "source": [
    "# Оценка энтропии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2coUXxYyAVx"
   },
   "source": [
    "## Автокодировщик\n",
    "\n",
    "Сжатие данных предлагается делать автокодировщиком.\n",
    "Для архитектуры специфицируется только формат входных данных, а также размерность внутреннего представления (кодов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aa1TECWvxuEF"
   },
   "outputs": [],
   "source": [
    "# РАЗМЕРНОСТЬ КОДА.\n",
    "# #\n",
    "# #\n",
    " \n",
    "codes_dim = 4\n",
    "\n",
    "# #\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3McJCha_b_G"
   },
   "outputs": [],
   "source": [
    "# Число эпох для обучения.\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOu7nB9gx9vH"
   },
   "outputs": [],
   "source": [
    "full_path = dataset_path + \"autoencoders/\" + str(codes_dim) + \"_\" + str(epochs) + \"/\"\n",
    "os.makedirs(full_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lwIjSKL_gMv"
   },
   "outputs": [],
   "source": [
    "info['dataset_dim'] = dataset_dim\n",
    "info['latent_dim'] = latent_dim\n",
    "\n",
    "info['samples_number'] = samples_number\n",
    "info['tests_number'] = tests_number\n",
    "\n",
    "info['codes_dim'] = codes_dim\n",
    "info['epochs'] = epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45W1qKkoB90y"
   },
   "source": [
    "### Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2475m-kB1mm"
   },
   "outputs": [],
   "source": [
    "def dense_autoencoder(shape_input, dimension):\n",
    "    # Инициализация весов.\n",
    "    init = tf.keras.initializers.RandomNormal(stddev = 0.02)\n",
    "\n",
    "    # Входные данные генератора / выборки.\n",
    "    input_layer = tf.keras.layers.Input(shape_input)\n",
    "    next_layer = input_layer\n",
    "    next_layer = tf.keras.layers.GaussianNoise(0.02)(next_layer)\n",
    "\n",
    "    # 1 блок слоёв.\n",
    "    next_layer = tfa.layers.SpectralNormalization(tf.keras.layers.Dense(64, kernel_initializer = init),\n",
    "                                                  power_iterations = 8)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "\n",
    "    # 2 блок слоёв.\n",
    "    next_layer = tfa.layers.SpectralNormalization(tf.keras.layers.Dense(32, kernel_initializer = init),\n",
    "                                                  power_iterations = 8)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    \n",
    "    # 3 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.Dense(8, kernel_initializer = init)(next_layer)\n",
    "    #next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    \n",
    "    # Бутылочное горлышко.\n",
    "    next_layer = tfa.layers.SpectralNormalization(tf.keras.layers.Dense(dimension),\n",
    "                                                  power_iterations = 8)(next_layer)\n",
    "    bottleneck = tf.keras.layers.Activation('tanh', name='bottleneck')(next_layer)\n",
    "\n",
    "    # Модель кодировщика.\n",
    "    encoder = tf.keras.Model(input_layer, bottleneck)\n",
    "\n",
    "    # Начало модели декодировщика.\n",
    "    input_code_layer = tf.keras.layers.Input((dimension))\n",
    "    next_layer = input_code_layer\n",
    "\n",
    "    # 3 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.Dense(8, kernel_initializer = init)(next_layer)\n",
    "    #next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    \n",
    "    # 2 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Dense(32, kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "\n",
    "    # 1 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Dense(64, kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    \n",
    "    # 0 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Dense(shape_input[0])(next_layer) # Подразумевается, что вход - всё равно вектор.\n",
    "    #next_layer = tf.keras.layers.Activation('tanh')(next_layer)\n",
    "    \n",
    "    output_layer = next_layer\n",
    "    \n",
    "    # Модель.\n",
    "    decoder = tf.keras.models.Model(input_code_layer, output_layer) # Декодировщик.\n",
    "    autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "\n",
    "    # Компиляция модели.\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 5e-3)\n",
    "    autoencoder.compile(loss = 'mse', optimizer = opt, loss_weights = [1.0])\n",
    "    \n",
    "    return encoder, decoder, autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzOjEmMjCCWS"
   },
   "source": [
    "### Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmzV6titB6zT"
   },
   "outputs": [],
   "source": [
    "#encoder = tf.keras.models.load_model(full_path + \"encoder.h5\")\n",
    "#decoder = tf.keras.models.load_model(full_path + \"decoder.h5\")\n",
    "#autoencoder = autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "#autoencoder.compile(loss = 'mse', optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3), loss_weights = [1.0])\n",
    "\n",
    "#with open(full_path + 'info.json', 'r') as fp:\n",
    "#    info = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ld3eyuWPCHtW"
   },
   "outputs": [],
   "source": [
    "encoder, decoder, autoencoder = dense_autoencoder((dataset_dim,), codes_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_callback = autoencoder.fit(samples, samples, epochs=epochs, validation_data=(tests, tests), batch_size=samples_number // 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(loss = 'mse', optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3), loss_weights = [1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(samples, samples, epochs=1000, validation_data=(tests, tests), batch_size=samples_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fB0qcPxALn0F"
   },
   "outputs": [],
   "source": [
    "# Сохранение динамики loss-функции.\n",
    "loss_history = np.array(history_callback.history[\"loss\"])\n",
    "val_loss_history = np.array(history_callback.history[\"val_loss\"])\n",
    "\n",
    "np.savetxt(full_path + \"loss.csv\", loss_history, delimiter=\"\\n\")\n",
    "np.savetxt(full_path + \"val_loss.csv\", val_loss_history, delimiter=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfZfumkbM4E_"
   },
   "outputs": [],
   "source": [
    "# Сохранение средних итоговых значений функции потерь.\n",
    "last_n = 20\n",
    "last_loss = np.array(loss_history[-last_n:])\n",
    "last_loss_mean = np.mean(last_loss)\n",
    "last_loss_std = np.std(last_loss)\n",
    "\n",
    "last_val_loss = np.array(val_loss_history[-last_n:])\n",
    "last_val_loss_mean = np.mean(last_val_loss)\n",
    "last_val_loss_std = np.std(last_val_loss)\n",
    "\n",
    "info['last_loss_mean'] = last_loss_mean\n",
    "info['last_loss_std'] = last_loss_std\n",
    "info['last_val_loss_mean'] = last_val_loss_mean\n",
    "info['last_val_loss_std'] = last_val_loss_std\n",
    "\n",
    "with open(dataset_path + \"losses.csv\", 'a+') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow([codes_dim, last_loss_mean, last_loss_std])\n",
    "\n",
    "with open(dataset_path + \"val_losses.csv\", 'a+') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow([codes_dim, last_val_loss_mean, last_val_loss_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQxF6KcgNag-",
    "outputId": "21f60548-9dd3-4430-a27e-3e871a431cfe"
   },
   "outputs": [],
   "source": [
    "# Сохранение моделей.\n",
    "autoencoder.save(full_path + \"autoencoder.h5\")\n",
    "encoder.save(full_path + \"encoder.h5\")\n",
    "decoder.save(full_path + \"decoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuImkbabNcqj"
   },
   "outputs": [],
   "source": [
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPMRQmWmNjIS"
   },
   "source": [
    "### Получение кодов всех элементов набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mksJnyPiNkhD"
   },
   "outputs": [],
   "source": [
    "codes = np.array(encoder.predict(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EMu1RqxN7vj"
   },
   "outputs": [],
   "source": [
    "codes_pca_dim = codes_dim\n",
    "PCA_codes = PCA(n_components=codes_pca_dim, whiten=True)\n",
    "codes_pca = np.array(PCA_codes.fit_transform(codes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8qbWLPNO5fg"
   },
   "source": [
    "### KDE для кодов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYPLhT5mO58e"
   },
   "outputs": [],
   "source": [
    "# Загрузка параметров KDE.\n",
    "\n",
    "#with open(full_path + 'info.json', 'r') as fp:\n",
    "#    info = json.load(fp)\n",
    "\n",
    "#kde_codes = KernelDensity(bandwidth=info['bandwidth'], kernel='gaussian')\n",
    "#kde_codes.fit(codes_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMOLm6vsO7Kb"
   },
   "outputs": [],
   "source": [
    "def smart_gridsearch(begin, end, resolution = 7, rel_x_epsilon = 0.01, rtol = 0.001, n_jobs = 2, cv = 5):\n",
    "    while True:\n",
    "        grid = np.logspace(np.log10(begin), np.log10(end), resolution)\n",
    "        print(\"Поиск по сетке: \", grid)\n",
    "        params = {'bandwidth': grid}\n",
    "        \n",
    "        grid_search = GridSearchCV(KernelDensity(rtol = rtol), params, n_jobs = n_jobs, verbose = 10, cv = cv)\n",
    "        grid_search.fit(codes_pca)\n",
    "        \n",
    "        if grid_search.best_index_ == 0:\n",
    "            begin *= begin / end\n",
    "            end = grid[1]\n",
    "        elif grid_search.best_index_ == resolution - 1:\n",
    "            end *= end / begin\n",
    "            begin = grid[-2]\n",
    "        else:\n",
    "            begin = grid[grid_search.best_index_ - 1]\n",
    "            end = grid[grid_search.best_index_ + 1]\n",
    "\n",
    "            if end - begin < rel_x_epsilon * grid[grid_search.best_index_]:\n",
    "                return grid_search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVCcZMpjPII5",
    "outputId": "3648b7da-ec10-446c-8ff0-1dee7f369aa9"
   },
   "outputs": [],
   "source": [
    "KDE_codes = smart_gridsearch(0.05, 0.2, n_jobs = n_jobs).best_estimator_\n",
    "KDE_codes.set_params(rtol = 0.0)\n",
    "print(KDE_codes.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90JoA1fZPLM8"
   },
   "outputs": [],
   "source": [
    "info['bandwidth'] = KDE_codes.get_params()['bandwidth']\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)\n",
    "\n",
    "with open(dataset_path + \"bandwidth.csv\", 'a+') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow([codes_dim, info['bandwidth']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtQMUyL7PTvt"
   },
   "source": [
    "## Подсчёт энтропии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hFnw6XaPXnb"
   },
   "outputs": [],
   "source": [
    "def _loo_step(bandwidth, samples, i):\n",
    "    loo_samples = samples\n",
    "    np.delete(loo_samples, i)\n",
    "    \n",
    "    kde = KernelDensity(bandwidth=bandwidth, kernel='gaussian')\n",
    "    kde.fit(loo_samples)\n",
    "    return kde.score_samples([samples[i]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB5i2VmjPcub"
   },
   "outputs": [],
   "source": [
    "def entropy_leave_one_out_parallel(path, bandwidth, samples, n_jobs = 2, first_N = None, parts = 10, recover_saved = False):\n",
    "    \"\"\"\n",
    "    Параллельное вычисление оценки энтропии методом убрать-один-элемент.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Создание временных папок для сохранения прогресса.\n",
    "    parts_path = path + \"LOO_PARTS/\"\n",
    "    os.makedirs(parts_path, exist_ok=True)\n",
    "\n",
    "    # Если дано first_N, энтропия будет оцениваться только на первых first_N элементах.\n",
    "    N = 0\n",
    "    if first_N is None:\n",
    "        N = len(samples)\n",
    "    else:\n",
    "        N = first_N\n",
    "\n",
    "    # Число частей и массив, их содержащий.\n",
    "    N_per_part = N // parts\n",
    "    log_probs = []\n",
    "\n",
    "    # Восстанавливаем прогресс, если требуется.\n",
    "    recovered_parts = 0\n",
    "    if recover_saved:\n",
    "        for filename in os.listdir(parts_path):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                log_probs.append(np.loadtxt(parts_path + filename))\n",
    "                recovered_parts += 1\n",
    "\n",
    "    print(\"Восстановлено блоков данных: %d\" % recovered_parts)\n",
    "\n",
    "    # Подсчёт логарифма вероятности в точках.\n",
    "    for part in range(recovered_parts, parts):\n",
    "        log_probs.append(\n",
    "            np.array(\n",
    "                Parallel(n_jobs = n_jobs, verbose = 10, batch_size = 8)(\n",
    "                    delayed(_loo_step)(bandwidth, samples, i) for i in range(part * N_per_part, min((part + 1) * N_per_part, N))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        np.savetxt(parts_path + str(part) + \".csv\", log_probs[part], delimiter=\"\\n\")\n",
    "    \n",
    "    # Объединение в один массив.\n",
    "    log_prob = np.concatenate(log_probs)\n",
    "\n",
    "    # Суммирование и нахождение стандартного отклонения.\n",
    "    average = -math.fsum(log_prob) / N    \n",
    "    squared_deviations = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        squared_deviations[i] = (log_prob[i] - average)**2\n",
    "    standard_deviation = np.sqrt(math.fsum(squared_deviations) / (N * (N - 1)))\n",
    "    \n",
    "    # Удаление временных файлов.\n",
    "    shutil.rmtree(parts_path)\n",
    "        \n",
    "    return average, standard_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvhNZC1IPiVO",
    "outputId": "96dfe559-ee14-4712-ae44-f8e76aac951d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_entropy, latent_entropy_error = entropy_leave_one_out_parallel(full_path, KDE_codes.get_params()['bandwidth'], codes_pca, n_jobs = n_jobs, recover_saved = False)\n",
    "print(\"LH: %f, errLH: %f\" % (latent_entropy, latent_entropy_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duowAmegPkzY"
   },
   "outputs": [],
   "source": [
    "info['latent_entropy'] = latent_entropy\n",
    "info['latent_entropy_error'] = latent_entropy_error\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fKF66FcPmAy"
   },
   "outputs": [],
   "source": [
    "with open(dataset_path + \"entropy.csv\", 'a+') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow([codes_dim, info['latent_entropy'], info['latent_entropy_error']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYmf7pj6YOcP"
   },
   "outputs": [],
   "source": [
    "def model_jac(x, model, m, n):\n",
    "    \"\"\"\n",
    "    Вычисление матрицы Якоби модели в точке.\n",
    "    \n",
    "    x - точка.\n",
    "    model - модель.\n",
    "    m - размерность входа.\n",
    "    n - размерность выхода.\n",
    "    \"\"\"\n",
    "    \n",
    "    variables = [tf.Variable([[x[j]]]) for j in range(m)]\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as t:\n",
    "        t.watch(variables)\n",
    "        z = tf.concat(variables, 1)\n",
    "        f = [model(z)[0][i] for i in range(n)]\n",
    "\n",
    "    J = np.zeros((m, n))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            J[j][i] = t.gradient(f[i], variables[j]).numpy()\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SY6yX8irXna6"
   },
   "outputs": [],
   "source": [
    "def defc_from_jac(J):\n",
    "    \"\"\"\n",
    "    Вычисление коэффициента растяжения для матрицы Якоби.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(J.shape) == 2\n",
    "    return np.sqrt(np.linalg.det(J @ J.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lP440XTulLNF"
   },
   "outputs": [],
   "source": [
    "def _defc_step(sample, model, m, n):\n",
    "    return np.log(defc_from_jac(model_jac(sample, model, m, n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdEkmydBj2fq"
   },
   "outputs": [],
   "source": [
    "def transform_entropy_nonparallel(path, samples, model, m, n, first_N = None, parts = 100, recover_saved = False):\n",
    "    \"\"\"\n",
    "    Однопоточное вычисление изменения энтропии под действием декодера.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Создание временных папок для сохранения прогресса.\n",
    "    parts_path = path + \"TE_PARTS/\"\n",
    "    os.makedirs(parts_path, exist_ok=True)\n",
    "\n",
    "    # Если дано first_N, энтропия будет оцениваться только на первых first_N элементах.\n",
    "    N = 0\n",
    "    if first_N is None:\n",
    "        N = len(samples)\n",
    "    else:\n",
    "        N = first_N\n",
    "\n",
    "    # Число частей и массив, их содержащий.\n",
    "    N_per_part = N // parts\n",
    "    log_defcs = []\n",
    "\n",
    "    # Восстанавливаем прогресс, если требуется.\n",
    "    recovered_parts = 0\n",
    "    if recover_saved:\n",
    "        for filename in os.listdir(parts_path):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                log_defcs.append(np.loadtxt(parts_path + filename))\n",
    "                recovered_parts += 1\n",
    "\n",
    "    print(\"Восстановлено блоков данных: %d\" % recovered_parts)\n",
    "\n",
    "    # Подсчёт логарифма вероятности в точках.\n",
    "    for part in range(recovered_parts, parts):\n",
    "        print(\"Новый блок: %d\" % part)\n",
    "        log_defcs.append(\n",
    "            np.array([_defc_step(samples[i], model, m, n) for i in range(part * N_per_part, min((part + 1) * N_per_part, N))])\n",
    "            )\n",
    "        np.savetxt(parts_path + str(part) + \".csv\", log_defcs[part], delimiter=\"\\n\")\n",
    "    \n",
    "    # Объединение в один массив.\n",
    "    log_defc = np.concatenate(log_defcs)\n",
    "\n",
    "    # Суммирование и нахождение стандартного отклонения.\n",
    "    average = math.fsum(log_defc) / N    \n",
    "    squared_deviations = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        squared_deviations[i] = (log_defc[i] - average)**2\n",
    "    standard_deviation = np.sqrt(math.fsum(squared_deviations) / (N * (N - 1)))\n",
    "    \n",
    "    # Удаление временных файлов.\n",
    "    shutil.rmtree(parts_path)\n",
    "        \n",
    "    return average, standard_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tlGygUXnJMU"
   },
   "outputs": [],
   "source": [
    "transform_entropy, transform_entropy_error = transform_entropy_nonparallel(full_path, codes, decoder, codes_dim, dataset_dim, first_N = 2000, recover_saved = False)\n",
    "print(\"TH: %f, errTH: %f\" % (transform_entropy, transform_entropy_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPbZFirnnljW"
   },
   "outputs": [],
   "source": [
    "info['transform_entropy'] = transform_entropy\n",
    "info['transform_entropy_error'] = transform_entropy_error\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5UgdsOMnsW7"
   },
   "outputs": [],
   "source": [
    "with open(dataset_path + \"entropy.csv\", 'a+') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow([codes_dim, info['transform_entropy'], info['transform_entropy_error']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBw5gOcqIF00"
   },
   "outputs": [],
   "source": [
    "# Коэффициент растяжения при денормализации.\n",
    "PCA_codes_defc = np.abs(np.linalg.det( PCA_codes.inverse_transform(np.eye(codes_pca_dim)) -\n",
    "                                       PCA_codes.inverse_transform(np.zeros((codes_pca_dim, codes_pca_dim))) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkjQc-SVIV_Q"
   },
   "outputs": [],
   "source": [
    "# Соответствующая энтропия.\n",
    "PCA_codes_transform_entropy = np.log(PCA_codes_defc)\n",
    "\n",
    "print(\"PCA_TH: %f\" % (PCA_codes_transform_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['PCA_codes_transform_entropy'] = PCA_codes_transform_entropy\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzRnFJODT5Kh"
   },
   "outputs": [],
   "source": [
    "# Итоговая оценка энтропии.\n",
    "entropy = latent_entropy + transform_entropy + PCA_codes_transform_entropy\n",
    "entropy_error = latent_entropy_error + transform_entropy_error\n",
    "\n",
    "print(\"H: %f, errH: %f\" % (entropy, entropy_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['entropy'] = entropy\n",
    "info['entropy_error'] = entropy_error\n",
    "\n",
    "info['true_entropy'] = true_entropy\n",
    "info['true_error'] = np.abs(entropy - true_entropy)\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifY4lCrVZogt"
   },
   "outputs": [],
   "source": [
    "print(\"Итоговый результат:\")\n",
    "print(\"Оценка: %f\\nИстинное значение: %f\\nОшибка: %.3e\" % (entropy, true_entropy, entropy - true_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Information v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
